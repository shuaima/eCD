\documentclass{letter}
\usepackage{geometry}

% duan
\usepackage{xspace}

\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}
\newcommand{\kwlog}{\emph{w.l.o.g.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\sstab}{\rule{0pt}{8pt}\\[-2.4ex]}



\begin{document}

Prof. Xuemin Lin,\\
Editor-in-Chief,\\
IEEE Transactions on Knowledge and Data Engineering\\

Dear Prof. Lin,

Attached please find a revised version of our submission to
IEEE Transactions on Knowledge and Data Engineering, \emph{An Ensemble
Approach to Link Prediction}.

The paper has been substantially revised according to the referees＊ comments.
In particular, (1) we have added an example for illustrating the procedure of
the top-$(\epsilon, k)$ prediction in Section 2.1, (2) we have added the
Resource Allocation (RA) algorithm for comparison in the experimental study,
(3) we have compared our methods with the state-of-the-art methods when
$k$ is fixed to the number of links in the ground truth data, (4) we have added
a set of tests for evaluating the ensemble-enabled approach with other link
prediction methods, \eg Adamic/Adar (AA) and RA,
and (5) we have also taken this opportunity to rewrite several parts of the
paper to improve the presentation.

We would like to thank all the referees for their thorough reading of our
paper and for their valuable comments.

Below please find our responses to the comments by the referees.

%******************* reviewer 1 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Referee 1.}

\textbf{[R1C1]} \emph{When talking about the recommendation, in addition to
the survey by Adomavicius and Tuzhilin, I recommend another survey from
physical society [L. Lu, et al., Recommender Systems, Phys. Rep. 519 (2012) 1-49],
in particular, this survey discussed the similarity and difference of link
prediction and personalized recommendation.}

We have added this survey in the related work. Thanks!

[33] L. L\"{u}, M. Medo, C. H. Yeung, Y. Zhang, and Z. Zhang. Recommender
systems. Physics Reports, 519:1-49, 2012.


\textbf{[R1C2]} \emph{In a sparse network, for most common neighborhood
based methods (e.g., CN, AA, Resource Allocation Index, Jaccard coefficient, etc.),
the time complexity is not $O(n^2)$, but $O(n*k^2)$, where $k$ is the average degree.
It is because for most node pairs, the CN-based similarities are zero.
Therefore, the corresponding statements should be modified.}

We have discussed the complexities of AA and RA in Section 4.1.


\textbf{[R1C3]} \emph{I can follow the operations to obtain S and R for
the efficient top-k search, however, the physical meaning of this approximation
is not clear. I strongly suggest the authors to show a clear picture,
for example, by illustrate the procedure for a small-size case link $n$ around 7,8
and r around 3,4. So that we could see the meaning of the method as well as
how to construct S and R step by step.}

We have added an example for illustrating the procedure of the
top-$(\epsilon, k)$ prediction in Section 2.1. Thanks for the suggestion!


\textbf{[R1C4]} \emph{How to determine $\theta$ when using the speeding up
method (Eq. 4 and Eq. 5). In addition, I do NOT think the proof of
Proposition 2 is necessary, since it is very obvious and provides
nothing more than Eq. 4 and Eq. 5.}

We have discussed the setting of the parameter $\theta$ in Section 3.6.
We kept the proof of Proposition 2 for illustrating the correctness of our
top-$(\epsilon, k)$ speeding up technique.



\textbf{[R1C5]} \emph{The edge bagging method is originally called snowball
sampling, please cite the original paper [P. Biernacki and D. Waldorf,
Snowball sampling: Problems and techniques of chain referral sampling,
Sociological Methods and Research 10 (1981) 141].}

The snowball sampling has been introduced in the graph sampling
survey [4] in our references. In addition, the edge bagging method
is different with the snowball sampling because the edge bagging only
selects a random adjacent node when the sampled node set is grown.


[4] N. K. Ahmed, J. Neville, and R. Kompella. Network sampling: From
static to streaming graphs. Transactions on Knowledge Discovery from
Data (TKDD), 8(2):7:1每7:56, 2014.


\textbf{[R1C6]} \emph{I personally like the biased edge bagging method,
and I agree with the statement "it often becomes more difficult to make
robust predictions between low-degree nodes". Please read and discuss
the following paper [Y. X. Zhu, Uncovering missing links with cold ends.
Physica A 391 (2012) 5769-5778.] that focuses on the related issue.
By the way, the authors are encouraged to test their novel bagging
methods for links with cold ends.}

Since the datasets we used have timestamps of edge arrivals, we focus on
predicting future links rather than missing links that randomly selects
some links as the testing set.


\textbf{[R1C7]} \emph{In addition to the related works from computer
science community, the authors should also highlight some important
works from physical science community, such as [Clauset, A., Moore, C., $\&$ Newman, M. E. (2008).
Hierarchical structure and the prediction of missing links in networks. Nature, 453(7191), 98-101],
[Guimer角, R., $\&$ Sales-Pardo, M. (2009). Missing and spurious interactions
and the reconstruction of complex networks. PNAS, 106(52), 22073-22078]
and [L邦, L., Pan, L., Zhou, T., Zhang, Y. C., $\&$ Stanley, H. E. (2015).
Toward link predictability of complex networks. PNAS, 112(8), 2325-2330.].}

Thanks for recommending the above works of link prediction. We have added
the link predictability [34] in the related work. For the lack of space,
we have cited the surveys [20][35] that introduce the rest.

[20] M. A. Hasan and M. J. Zaki. A survey of link prediction in social
networks. In Social Network Data Analytics, 2011.

[34] L. L邦, L. Pan, T. Zhou, Y. Zhang, and H. Stanley. Toward link
predictablility of complex networks. PNAS, 118(8):2325每2330, 2015.

[35] L. L邦 and T. Zhou. Link prediction in complex networks: A survey.
Physica A, pages 1150每1170, 2011.

\textbf{[R1C8]} \emph{Please compare the prediction accuracy of current
bagging methods with some other methods like resource allocation index
[Zhou, T., L邦, L., $\&$ Zhang, Y. C. (2009). Predicting missing links via
local information. The European Physical Journal B-Condensed Matter and
Complex Systems, 71(4), 623-630.], loop model [Pan, L., Zhou, T., L邦, L., $\&$ Hu, C. K. (2016).
Predicting missing links and identifying spurious links via likelihood
analysis. Scientific reports, 6, 22955.] and the above mentioned three
methods (see comment 7, Nature 2008, PNAS 2009 and PNAS 2015), in some
smaller size networks. Therefore we get more clear picture how accurate
the present method is.}

We have added RA as a comparison algorithm in the experimental study. However,
we did not adopt other algorithms for comparison since the complexities of these
algorithms are at least $O(n^2)$, which makes it difficult to apply them on large
networks with millions of nodes. Thanks for the suggestion!


%******************* reviewer 2 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Referee 2.}

\textbf{[R2W1]} \emph{Due to the fact that the decomposition is based
on random sampling, thus authors should provide the 95\% confidence
intervals of performance. This is important to demonstrate that the
proposed methods are significantly better than the state-of-the-art
methodology.}

We have added two tables containing the accuracy together with its
95\% confidence intervals of our methods compared with the
state-of-the-art methods in Section 4.2.1 and the supplementary material
respectively. Thanks for the suggestion!


\textbf{[R2W2]} \emph{Authors are using top-k precision to evaluate
the link prediction results. However, the measurement is not used
correctly. Top-k precision will give a fair comparison between different
link prediction methods only when the K is well selected. Please refer
to the paper "Evaluating link prediction methods". Top-k precision
measurement can have different conclusions on link prediction results
when there are some small changes of the value K. K should be the
number of true links in test set (fixed). In this paper, authors
mentioned that the K range from $10^4 - 10^5$, this is not a correct
way to employ this measurement.}

Since we focus on the top-$k$ ranking problem for link prediction, we
intuitively adopt the top-$k$ precision for evaluation. We agree with
that small changes of the value $k$ may lead to different conclusions,
so we tested the impact of varying $k$ in the experimental study.
Moreover, we tested the proposed
ensemble-enabled approach when $k$ is fixed to the number of links in
the ground truth data. The results are reported in the supplementary material
and indicate the robustness and efficiency of our methods compared with other
methods. One thing should be noted that the datasets used
in our experiments are more sparse than that used in [1], which means that it
is more difficult to predict links in our datasets. As a result, the top-$k$
precision on our datasets, when $k$ is fixed to the number of links in the
ground truth data, is too small to be used in practice. Therefore, we maintain
the default values for $k$ in the experimental study according to [39][43][47].

[1] Y. Yang, R. N. Lichtenwalter, and N. V. Chawla. Evaluating link prediction
methods. Knowl Inf Syst, 45:751每782, 2015.

[39] D. Song, D. Meyer, and D. Tao. Top-k link recommendation in social
networks. In ICDM, 2015.

[43] R. West, A. Paranjape, and J. Leskovec. Mining missing hyperlinks from
human navigation traces: A case study of wikipedia. In WWW, 2015.

[47] S. Zhai and Z. Zhang. Dropout training of matrix factorization and
autoencoder for link prediction in sparse graphs. In SDM, 2015.

\textbf{[R2W3]} \emph{AA can only predict two hops links, it has no
predictive power to infer any potential links with larger hop distance.
Thus to provide a fair comparison, authors should provide the performance
comparison in each hop distance, namely hop 2, hop 3, and etc.
An example can be found in "Evaluating link prediction methods".}

We have added an accuracy comparison in each hop distance in Section 4.2.4.
Thanks for the suggestion!


\textbf{[R2W4]} \emph{Authors proposed a framework to decompose the
large scale network link prediction problem, thus the latent factor
model can be replaced with some other link prediction methods. So
what if we replace latent factor model with the AA method, how is
the performance? Whether the ensembled AA results yield comparable
performance than the AA in the original network?}

We have added an explanation in Section 3.6 to declare that the ensemble-enabled
approach is not only designed for NMF, but also applied to any other link
prediction methods. Moreover, we revised the bagging+ and bagging methods by
replacing NMF with AA and RA and evaluated their performance in the
supplementary material.


%******************* reviewer 3 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Referee 3.}

\textbf{[R3C1]} \emph{The section could be preceded by clearly noting
the two challenges (prediction/training complexity) in large networks,
and how separate tools are used to handle each.}

We have added the remarks to explain the two challenges and how to handle
them by latent fact models and top-$(\epsilon, k)$ prediction searching in
Section 2.


\textbf{[R3C2]} \emph{The basic model proposed is $W = FF^T$. This is
quite sensible, but in comparison with the eigendecomposition,
one misses the diagonal matrix of possibly negative weights.
A comment on the limitation of this assumption may be prudent. See also
Peter Hoff. Modeling homophily and stochastic equivalence in symmetric
relational data. NIPS 2008.}

We adopt NMF because it requires only $O(nr^2)$ time to be trained and
it generates non-negative and sparse factorized matrices [24], which could
be searched efficiently.

%Moreover, NMF gives latent space an parts-based
%interpretation. Each dimension of latent space could be interpreted by a
%part of user's attributes, such as current city, hometown, personality and
%so on. It is more intuitive to represent each user as an additive mixture of
%attributes rather than represent each user as a combination of different
%representatives [51].


[24] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative
matrix factorization. Nature, 401:788每791, 1999.

%[51] L. Zhu, D. Guo, J. Yin, G. V. Steeg, and A. Galstyan. Scalable temporal
%latent space inference for link prediction in dynamic social networks.
%TKDE, 28(10):2765每2777, 2016.

\textbf{[R3C3]} \emph{The remark that absent links may be interpreted
as noisy ones is a reasonable one. There has been relevant work on
dealing with such noisy data in a matrix setting, e.g. Cho-Jui Hsieh,
Nagarajan Natarajan, Inderjit Dhillon. PU Learning for Matrix Completion.
ICML 2015. This is perhaps more prudent that directly fitting the value 0
for absent links.}

Since the social networks are often sparse, we fit the value 0 for absent links,
which simplifies the framework of latent factor models and provides an
intuitive way for predicting links. Thanks!


\textbf{[R3C4]} \emph{A citation should be provided for the claim
that NMF usually results in sparse F. In standard matrix factorisation
models the latent weights are usually completely dense.}

We have added a citation [24] that shows the factorized matrices of NMF
are usually sparse. Thanks for the suggestion!

[24] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative
matrix factorization. Nature, 401:788每791, 1999.

\textbf{[R3C5]} \emph{The explanation of the top-K prediction component
could be considerably improved. At present, the algorithm is presented,
and correctness shown, but one is not left with a lot of intuition for
what exactly is being done, and how it speeds up the computation. My
current understanding is that one views the computation of scores for a
pair (i, j) via
$W_{ij} = F_i F_j^T = S_i S_j^T = \sum_{p} S_{ip} * S_{jp} = \sum_{p} B^{p}_{ij}$
for a suitable set of matrices $B^{p}$. And that one then looks to compute
only some of the entries of this matrix, with the rest treated as zero.
However, in this view it is unclear why the loop over j does not include
the nodes with score greater than $\sqrt{(eps/r)}$; in fact what happens
when all nodes satisfy this condition is a bit unclear.}

To illustrate the procedure of the top-$(\epsilon, k)$ prediction clearly,
we have added an example in Section 2.1. Thanks!

\textbf{[R3C6]} \emph{In Proof of Prop 1, it is worth commenting that
when we break the loop, we are guaranteed that $S(i,p) * S(j',p) < e/r$
for all further j', since the S are sorted.}

We have added the commentary in the Proof of Proposition 1.
Thanks for the suggestion!

\textbf{[R3C7]} \emph{In discussing the motivation, comment could be
made about the complexity of stochastic gradient training of latent
factor models.}

We have added the complexity of training of latent factor models in
Section 3. Thanks!

\textbf{[R3C8]} \emph{Comment could be made that in principle, the
methods proposed here could be applied to any link prediction method,
and that there is nothing latent factor specific.}

We have added an explanation in Section 3.6 to declare that the ensemble-enabled
approach is not only designed for NMF, but also applied to any other link
prediction methods. Moreover, we implemented ensemble-enabled methods with AA and RA
and evaluated their performance in the supplementary material.
Thanks for the suggestion!



\textbf{[R3C9]} \emph{The Flickr dataset only seems to be used
in Sex 4.2.5, and not the other experiments. If this is indeed so,
there does not appear to be an explanation of why.}

The Flickr dataset in used to illustrate the limitation that 
bagging methods may reduce the prediction accuracy when the pairwise
overlapping of predicted links is high, \ie the diversity of ensembles
is lower. We think this result is useful for instructing uses to apply
the bagging methods in a correct way.


\textbf{[R3C10]} \emph{4.2.1 and 4.2.4 seem like they should be merged,
as both deal with the comparison of the same two groups of methods.}

We have merged the Section 4.2.1 and 4.2.4. Thanks!

\textbf{[R3C11]} \emph{On the presentation side, below are a few
comments that may improve the final manuscript:\\
- perhaps using $\bar{f}_i$ rather $\bar{F}_i$ would be clearer\\
- use $\backslash$left( and $\backslash$right) for Eqn 3\\
- use $\cos$, $\log$, $\exp$\\
- Prop 3, "IS included"\\
- Sec 3.5, "pretty good result" is informal\\
- Sec 4, "impacts of various factors" seems vague\\
- Sec 4.1, "largest AA SCORING"\\
- Sec 4.1, "that THE more communities"\\
- Sec 4.1, "GIVEN F, the BIGCLAM"\\
- Sec 4, consider expanding lists of conclusions of the form (a) ..., (b) ..., so that each bullet starts on a new line.\\
- Exp 1.2, "methods SCALE better than"}

We have fixed these typos. Thanks!


\line(1,0){500}

Your sincerely,

Liang Duan, Charu Aggarwal, Shuai Ma, Tiejun Ma, Jinpeng Huai

\end{document}
