\documentclass{letter}
\usepackage{geometry}

% duan
\usepackage{xspace}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{multirow,amsmath, array,colortbl}


\newcommand{\marked}[1]{\textcolor{red}{#1}}

\newcommand{\kw}[1]{{\ensuremath {\mathsf{#1}}}\xspace}

\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}
\newcommand{\kwlog}{\emph{w.l.o.g.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\sstab}{\rule{0pt}{8pt}\\[-2.4ex]}

\newcommand{\topk}[1]{\kw{top}--\kw{#1}}
\newcommand{\topdown}{\kw{topDown}}
\newcommand{\extsubgraph}{\kw{compADS^+}}
\newcommand{\drfds}{\kw{FIDES^+}}
\newcommand{\extsubgraphold}{\kw{compADS}}
\newcommand{\findtimax}{\kw{maxTInterval}}
\newcommand{\findtimin}{\kw{minTInterval}}
\newcommand{\meden}{\kw{MEDEN}}

\newcommand{\tranformgraph}{\kw{convertAG}}
\newcommand{\mergecc}{\kw{strongMerging}}
\newcommand{\strongpruning}{\kw{strongPruning}}
\newcommand{\boundedprobing}{\kw{boundedProbing}}

\newcommand{\AFPR}{\kw{AFP}-\kw{reduction}}
\newcommand{\nwm}{{\sc nwm}\xspace}


\newcommand{\cone}[1]{{$\mathcal{C}{#1}$}}
\renewcommand{\circle}[1]{{$\mathcal{O}{#1}$}}
\newcommand{\pcircle}[1]{{$\mathcal{O}^c{#1}$}}

\newcommand{\vv}{\overrightarrow}


\newcommand{\todo}[1]{\textcolor{red}{Todo...#1}}
\begin{document}





Prof. {Chris Jermaine} \\
Editor-in-Chief		\\
ACM Transactions on Database Systems (TODS)	\\



Dear Prof. Jermaine,

Attached please find a revised version of our submission to
TODS, \emph{Error Bounded Line Simplification Algorithms for Trajectory Compression: An Experimental Evaluation}.


 This article has been substantially revised. In particular, we have
	(a) added the weak simplification algorithms OPERB-A and \kw{CISED}-{W} that allow data interpolations for evaluation,
	(b) added the online algorithm \kw{DOTS} that uses the Local Integral Square SED (LISSED) as the error measure for evaluation,
	(c) added a new set of experimental tests on an additional application, named ``when\_at'' queries,
	(d) adopted a larger dataset of UCar in the experimental studies,
	(e) added a brief introduction to the semantic based trajectory compression methods in the appendix,
    (f) added an explanation on why map-matching is not considered as a pre-processing step of trajectory simplification, and
    (g)  refined the entire article to improve its readability.


We would like to thank all the referees for their thorough reading of our article and for their valuable comments.

Below please find our detailed responses to the comments.



%******************* reviewer 1 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 1.}

\line(1,0){100}


\textbf{[R1C1]} \emph{What is ``min-\# problem" mentioned in Section 1 (second paragraph)?}


The \emph{$min-\#$ problem} of trajectory simplification is, given a curve (\ie an original trajectory) and an error bound $\epsilon$, to construct an approximate curve that has the error bounded within $\epsilon$ and the minimum number of line segments.

We have explained it in Section 1 (the second paragraph), together with an explanation of the \emph{min-$\epsilon$} problem (which is further required by R3C4(3), that is, given $m$, construct an approximate curve consisting of at most $m$ line segments with the minimum error).
Thanks for pointing out this!

\textbf{[R1C2]} \emph{It is stated in the paper that ``The idea of piece-wise line simplification (LS) comes from computational geometry, whose target is to approximate a fine piece-wise linear curve with a coarse one (whose corresponding data points are a subset of the original one), such that the maximum distance of the former to the latter is bounded by a user specified threshold." This is actually misleading. Line simplification actually could be performed by using points other than original ones to represent the original line. The authors did mention those algorithms which consider points outside the original trajectories in the appendix. However, to make sure the correctness of the statements, it's better to mention that in the beginning of the paper. In addition, it is necessary to explain why this paper only considers the line simplification algorithms that consider the original points only. }

Yes, line simplification can use extra data points, not necessarily the original ones, to represent the original trajectory. Indeed, there are \emph{strong simplification} that outputs data points that must belong to the original input trajectory, and \emph{weak simplification} that allows to interpolate new data points in the simplified trajectory.

(1) We have corrected the statements at the beginning of the article (the second paragraph of Section 1), where ``(whose corresponding data points are a subset of the original one)" is replaced by ``If all data points of the coarse curve are a
subset of the original curve, then it is referred to as strong simplification; otherwise, it is referred to
as weak simplification [56])''.

(2) Now, this article has covered both strong and weak simplifications, and we have tested and analyzed ``weak simplification" algorithms (OPERB-A and CISED-W) in Section 6.

Thanks for pointing out this!

\textbf{[R1C3]} \emph{For trajectories that capture the moving objects' movement along the road network, the points are normally mapped to road networks. In other words, it is not necessary to use piece-wise line representation. It looks like the authors have not considered this case, which is actually one of the most common cases. Can the authors please explain why map-matching step is not considered even for trajectories that capture the movements in road networks?}

%step 1, two kinds of compression methods, orthogonal to each other.
Yes, for a moving object in urban areas, its trajectory could be mapped onto the road network. We have also observed that there is a class of trajectory compression called road-aware trajectory compression [Song:Press, Gotsman:Dilution] that matches data points onto roads  for the objects moving along the road networks, and  have introduced these works in the appendix. The road-aware trajectory compression belongs to semantic based trajectory compression, which is orthogonal to line simplification, \ie for trajectories that capture the moving objects' movement along the road network, they can be compressed by some road-aware trajectory compression method that involves the map-matching step, or alternatively by some line simplification method that does not need the map-matching step. %Both methods are technically applicable to this scenario.

%For road-aware trajectory compression, it is able to represent a trajectory basing on long paths (a path is a sequence of roads connected one by one) and further mine the frequency patterns, so as to get the overall compression of trajectories; For line simplification, it is light ...

We next specifically explain, in this evaluation, ``\emph{why map-matching step is not considered even for trajectories that capture the movements in road networks}?''
The reasons are two folds.
% Indeed, this question is related to ``why this paper focuses on line simplification and let alone map-matching".

(1) Line simplification is light, efficient and easy to implement. It is suitable to run on those resource-constrained end devices, on which map-matching is heavy or even not applicable, as it does not require any extra information like road networks. As we know, trajectory simplification is typically better to be performed as early as possible. That is, it would be better to run trajectory simplification on mobile devices that collect GPS points, such that not only the storage of data servers but also the network bandwidth between mobile devices and data servers is saved. In this case, line simplification without map-matching is a better choice than road-aware method.

(2) It is a convenient, and somehow indispensable, way to directly represent the trajectory by line segments at the first (even on the sever-side). Moving objects often move in areas where no fine or up-to-date road network information is available, \eg the dataset Geolife includes some moving object alternately taking cars, trains and airplanes, and by feet, and a part of its data points are not on any road or they are on roads whose information is not available. In these cases, if we roughly match the trajectory onto road network at the beginning, then it might introduce new errors caused by map-matching. However, if we save the (simplified) line representation of the trajectory, then once the road network is available or updated, it can also conveniently be matched onto roads at the time that we need later. Indeed some road-aware studies have already applied this strategy, \eg the dilution-matching-encoding  method [Gotsman:Dilution], which first simplifies the trajectories by some line simplification method, then maps the simplified trajectories onto roads.
%Anyway, it is an effective and even indispensable way to directly represent the trajectory by (simplified) line segments at the first (even on the sever-side).
%An line representation (original or simplified) of a trajectory can be matched onto road networks at anytime.


That is, in this article, we are considering the methods that require no extra knowledge for trajectory simplification, where there already exist quite a few materials to cover. We have further clarified this in the second paragraph of Section 1. Thanks for raising this question!
%Thus, even for trajectories that capture the movements in road networks, we will compress the trajectories by some line simplification method and do not consider the map-matching step (if needed, it could be the subsequent step of line simplification), and in this evaluation, \emph{we focus on line simplification and let alone semantic-based trajectory compression methods (including road-aware methods based on map-matching)}.

%, say, {``It is a great benefit that line simplification is suitable to run on resource-constrained end devices, such that the trajectories could be simplified at the early time, meaning not only the storage of data servers but also the network bandwidth between the end devices and the data servers is saved. Besides, line simplification could combine with other technologies, like the dilution-matching-encoding method that maps the simplified trajectories to road networks, then mine and use high frequency patterns of compressed trajectories instead of roads, so as to further improve the effectiveness of trajectory compression."} Moreover, a discussion of additional trajectory compression methods (including the road-aware trajectory compression) is provided in the appendix.

%To clearify this, we have added a brief introduction to map-matching-based trajectory compression techniques in ``Appendix: Additional Trajectory Compression Algorithms".


[Song:Press] R. Song, W. Sun, B. Zheng, and Y. Zheng. Press: A novel framework of trajectory compression in road networks. PVLDB, 7(9):661--672, 2014.

[Gotsman:Dilution] R. Gotsman and Y. Kanza. A dilution-matching-encoding compaction of trajectories over road networks. GeoInformatica 19(2), 331--364, 2015.

%[r3] H. Elmeleegy, A. K. Elmagarmid, E. Cecchet and W. G. Aref. Online Piece-wise Linear Approximation of Numerical Streams with Precision Guarantees. PVLDB, 2009.


\textbf{[R1C4]} \emph{ In Page 6 (line 19), it is stated ``all points in the simplified trajectory belong to the original trajectory"? Why? If the error bound is the requirement, why we are not allowed to introduce new points, if it helps to improve the compression rate without violating the error bound defined? This is related to my comment C1. }

Yes, we have allowed interpolating new points into the output trajectories, and we have corrected this in the last paragraph of Section 2 (please also refer to [R1C2] for more details). Thank you!


\textbf{[R1C5]} \emph{The three distance metrics used in this paper are mainly for line simplification. However, if we apply line simplification to compress trajectories, some of the metrics are no longer that useful. For example, perpendicular distance metric (PED) does not consider temporal dimension, which means the error bounds based on PED consider spatial distances only. However, temporal dimension is very important to trajectories. I think it is necessary to revisit the metrics and highlight their limitations. Particularly, the error-bounds provided by these metrics could be misleading as the metrics do not fully reflect the loss of information. }

(1) Yes! When PED is introduced to simplify trajectories, the input trajectory is treated as a sequence of spatial data points, and line simplification algorithms using PED bring good compression ratios at a cost of losing temporal information of trajectories. Hence, it is not spatio-temporal \emph{query friendly}.
However, PED still has its usages, \eg trajectory simplification using PED often serves as a pre-processing step of trajectory clustering that is the base of applications like traffic pattern recognition and urban planning [Zhao:Clustering, Mazimpaka:Mining].

(2) The SED of a point to a line segment is the Euclidean distance between the point and its \emph{synchronized point} \wrt the line segment, the expected position of the moving object on the line segment at the same time \emph{with an assumption that the object moves straightly along the line segment at a uniform speed}. {Most of these algorithms ensure that the \emph{maximal} SED from the output trajectory to the input trajectory is bounded by a SED error bound.}
They friendly support spatio-temporal queries like \emph{where\_at}. {Note that given the same error bound, algorithms using SED typically produce more points than PED as they preserve temporal information.}

(3) DAD is the direction deviation of the moving object, and
the temporal information is also lost when using DAD, therefore, it is \emph{not spatio-temporal query friendly} too. Nevertheless, it is important for applications such as direction-based trajectory clustering and query processing [Long:Direction].

To clarify this, we have revised the introduction of distance metrics PED, SED and DAD, and have also highlighted their limitations in the part of ``distance metrics'' of Section 1. Thanks!

[Zhao:Clustering] Zhao, L., and Shi, G. A trajectory clustering method based on douglas-peucker compression and density for marine traffic pattern recognition. Ocean Engineering 172 (2019), 456--467.

[Mazimpaka:Mining] Mazimpaka, J. D., and Timpf, S. Trajectory data mining: A review of methods and applications. Journal of Spatial Information Science 13 (2016), 61--99.

[Long:Direction] Long, C., Wong, R. C.-W., and Jagadish, H. Direction-preserving trajectory simplification. PVLDB 6, 10 (2013), 949--960.

\textbf{[R1C6]} \emph{ The reason that we decide to store the trajectories (or the compressed version of raw trajectories to save storage space) is mainly to support some applications if required later, which might not require $100\%$ accuracy. For example, in order to find out the witness of a fatal car accident, police might want to locate all the drivers who pass by a specific location within a given time window by querying the trajectories. I am not very sure whether the error bounds based on different distance metrics are still valid for specific types of queries. {This survey does not consider the applications to be supported by the compressed trajectories at all. However, personally I think it is very important.} The authors did include one set of experiments towards the end of the paper. However, I think it is very important to explain the criteria of trajectory compression in the beginning of the paper, to offer the audience a complete image. In this paper, what are the key considerations when we compress trajectories? Is the compression ratio the only requirement? Is it necessary to support certain applications with error bounds? }


These comments are largely talking about two kinds of related questions. First, whether the error bounded line simplification algorithms coupling with distance metrics are still valid for specific types of applications? Secondly, what are the quality criteria of the evaluation? We should answer these questions one by one.

(1) \emph{The error bounded line simplification algorithms coupling with different distance metrics not only reduce data sizes such that the storage, network and computing resources are saved, but also support their specific applications}. For SED, authors in [Cao:Reduction] have proved that the SED-simplified trajectories are answer-error-bound for spatio-temporal queries (\emph{query friendly} in short) of \emph{where\_at}, \emph{intersect} and \emph{nearest\_neighbor}.  %\ie if the simplification is SED-error bounded by $\epsilon$, then the answer to such a query on the simplified trajectory is also error bounded by $\epsilon$. Besides, the example in your comment, say, ``\emph{in order to find out the witness of a fatal car accident, police might want to locate all the drivers who pass by a specific location within a given time window by querying the trajectories}", is indeed a case of the spatio-tempporal \emph{intersect} query (also called a spatio-temporal \emph{range} query) [Cao:Reduction], defined in $intersect$ ($T$, \emph{Poly}, $t_1$, $t_2$), that returns \emph{true} if the trajectory $T$ intersects the polygon \emph{Poly} between the times $t_1$ and $t_2$. Obviously, it's supported by the error-bounded line simplification algorithms using SED.

For PED and DAD, authors in [Cao:Reduction] have also proved that PED is not spatio-temporal query friendly, and PED or SED neither is friendly to the ``when\_at" query. Though DAD is not discussed in [Cao:Reduction] (DAD is developed after the publish of [Cao:Reduction]), it is not spatio-temporal query friendly too. Our tests in Section 6.3.5 also support these statements. Nevertheless, \emph{the error bounded line simplification using PED or DAD also has its usages}, \eg it's often the pre-processing step of trajectory clustering that is the base of applications like urban plan and traffic pattern recognition [Zhao:Clustering, Mazimpaka:Mining]. %, where rather the shape of a trajectory than the exact positions of the individual are concerned.


Indeed, different applications may have different requirements, and the purpose of this evaluation is exactly to provide guidelines on how to choose appropriate algorithms and distance metrics for practical applications.


(2) {Yes, it is important to explain the criteria of trajectory compression to offer the audience a complete image.} Thus, \emph{we have added a part of content named ``Quality criteria"} in Section 1. Indeed, when we talking about the quality criteria of trajectory simplification, there are two levels,
(a) the first level comes from trajectory simplification itself, including compression ratio, efficiency and errors of simplification. Most works in this area test their algorithms following a part or all of these criteria, and
(b) the second level is from trajectory applications, \eg spatio-temporal queries, map-matching, trajectory clustering, anonymous and so on. Indeed, every trajectory application may have its own requirements to choose the right simplified trajectories, \eg spatio-temporal queries concern more about the answer-errors and a map-matching or a trajectory clustering method concerns more about the accuracy.

In this article, we first follow the mainstream works in this area and test the compression ratios, efficiency and errors of simplification algorithms.
Besides, as the data aging problem is a potential challenge to the management of trajectories, thus, the average and max errors of trajectory simplification algorithms as well as distance metrics in data aging are also tested.
%
Then for trajectory applications, we choose spatio-temporal queries as the representatives as they are well-known and commonly used in the management of trajectories.
More specifically, we have evaluated line simplification algorithms and distance metrics with respect to spatio-temporal queries \emph{where\_at} and \emph{when\_at}, two fundamental blocks of spatio-temporal queries. Note that [Cao:Reduction] have proved that ``the errors of the \emph{intersect} and \emph{nearest\_neighbor} query types are bounded if and only if the error of \emph{where\_at} is bounded", which indicates that the \emph{where\_at} query is one of the keys to test spatio-temporal queries; and \emph{when\_at} is another important functional block that is often discussed when comparing with \emph{where\_at}.

(3) For the last three sub-questions, we believe these are the key considerations when we compress trajectories. Compression ratios are surely not the only requirement as mentioned above, but it is a very important metric to support certain applications with error bounds, \eg spatio-temporal queries of \emph{where\_at}, \emph{intersect} and \emph{nearest\_neighbor}.


We have clarified these in the bullets of ``distance metrics'' and ``quality criteria'' of Section 1, and added new tests about \emph{when\_at} in Section 6.3.5. Thanks for your great advice!


[Cao:Reduction] Cao, H., Wolfson, O., and Trajcevski, G. Spatio-temporal data reduction with deterministic error bounds. VLDBJ 15, 3 (2006), 211--228.




\textbf{[R1C7]} \emph{ The definition of Error Bounded Algorithms is not precise as the error bounds are not presented. }

We have revised the definition of error bounded algorithms in the penultimate paragraph of Section 2. That is, given a trajectory $\dddot{\mathcal{T}}\left[P_0, \dots, P_n\right]$ and a pre-specified bound $\epsilon$,
trajectory simplification algorithm $\mathcal{A}$ using PED (respectively, SED and DAD) is \emph{error bounded} by $\epsilon$ if for each point $P_k$ ($k\in[0,n]$) in $\dddot{\mathcal{T}}$, there exists a line segment $\mathcal{L}_i = \vv{P'_{s_i}P'_{e_i}}$ in $\overline{\mathcal{T}}$ with $s_i \le k \le e_i$ ($0\le i\le m$) such that the PED distance $ped\left(P_k, \mathcal{L}_i\right)$  (respectively the SED distance $sed\left(P_k, \mathcal{L}_i\right)$ and the DAD distance $dad\left(\vv{P_{k}P_{k+1}}, \mathcal{L}_i\right)$) is no more than  $\epsilon$.

Thanks for pointing out this!


\textbf{[R1C8]} \emph{ Fig.2 in Page 7 gives an example of reachability graph. However, that graph is incomplete. For example, there is no link between $P_0$ and $P_5$ but they are reachable. This is very misleading. }

Yes, to avoid the misleading, we have moved up the point $P_4$ in Figures 1-3, 5-8 and 10 such that the PED distances from $P_4$ to line segments $\overline{P_0P_5}$, $\overline{P_1P_5}$, $\overline{P_2P_5}$ and $\overline{P_3P_5}$ are clearly all larger than the error bound. Besides, all running examples have been revised.
Thanks for raising this issue!

%\begin{center}
%	%\includegraphics[width=1.\textwidth]{example-image}
%	\includegraphics[scale=0.75]{reachgraph.png}
%	\label{fig:rgraph}
%\end{center}

%As shown below, if we connect $P_0$ and $P_5$, then the \kw{PED} from $P_4$ to line segment $\overline{P_0P_5}$, \ie $|P_4P'_4|$, is a bit larger than the error bound $\epsilon$ (exact two frames in the figure). Thus, $P_5$ is not reachable from $P_0$, and there should be no link between them.




\textbf{[R1C9]} \emph{ The datasets used in the experimental study were rather small. With 20M+ points, there is no need to compress them. Much larger datasets will be more realistic for the topic of trajectory compression.  }

First, the computing complexities of a line simplification algorithm are tightly related to the length (number of points) of the input trajectory rather than the number of trajectories, and our datasets have already included long trajectories that each has more than {one million} data points. This is quite memory consuming for optimal and batch algorithms  as they need to load the entire trajectory into memory, and time consuming for batch algorithms as they need {days} to finish the tests, as shown in Section 6.

Second, we have extended dataset UCar from 200 to 1000 trajectories such that it has more than {100M} points in total (a data point has tens of bytes, including object id, latitude, longitude, height, time and other information). Besides, we have already used the full datasets of Geolife and Mopsi, which are widely used in many recent trajectory compression studies, \eg Geolife is used in the recent algorithm CISED [Lin:CISED] and experimental study [Zhang:Evaluation], and Mopsi is used in [Lin:CISED].  %Thus, an even larger datasets is quite not friendly to the tests.
%Thanks for your advice!

[Lin:CISED] Lin, X., Jiang, J., Ma, S., Zuo, Y., and Hu, C. One-pass trajectory simplification using the synchronous euclidean distance. VLDBJ 28, 6 (2019), 897 --921.

[Zhang:Evaluation] Zhang, D., Ding, M., Yang, D., Liu, Y., Fan, J., and Shen, H. T. Trajectory simplification: An experimental study and quality analysis. PVLDB 9, 11 (2018), 934--946.

\textbf{[R1C10]} \emph{The average errors introduced in Page 20 are derived based on ONLY points contained in a line segment of a piece-wise line representation. Why? Why not consider all the points in the original trajectories? }

Indeed, we do consider ALL the points in the original trajectories when calculate the average errors. To clarify this, we have revised the description of ``average errors" in Section 6.2  to ``{The average simplification error is the average value of the distances from every point of the original trajectories to its representing line segment of the simplified trajectories}''. Thanks for pointing out this!


\textbf{[R1C11a]} \emph{ In Page 21 (lines 18-19), it is stated that ``We then choose 10 trajectories from each dataset, and vary the size |T| of a trajectory from 1,000 points to 10,000 points while fixing the error bound $\epsilon = 40$ metres or $\epsilon = 45$ degrees". It is not clear how points are selected. For example, a raw trajectory contains 100 points (p1, p2, ..., p100). If we want to only consider 10 points, do you consider a sub-trajectory of 10 points (say p1, p2, ..., p10) or a trajectory of much lower sampling rate (say p1, p11, p21, ..., p91). Please state it clearly!}

It is the first K points of a trajectory (\eg, the first 100 points are $P_1, P_2, P_3, ..., P_{100}$). We have clarified this in the last paragraph of Section 6.1. Thanks!

\textbf{[R1C11b]} \emph{ In Page 21 (lines 44 - 46), it is stated that ``The dataset collected by cars (e.g., UCar) also has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cares typically move more regularly than individuals." However, this is different from our everyday observation. Because of the traffic light, cars are not expected to move so regularly. Second, it is different from the observations we could make from Figures 15, 16, 17, 18, 19, and 20. Based on the results reported in those six figures, compression ratios under Geolife and Mopsi are lower than that under UCar, which means the dataset collected by cars has poorer compression ratios. Third, it shall be 'cars' but not 'cares'. }

Here, the word ``regular'' with respect to the movement of a car or an individual has two meanings: (1) regular in speed and regular in direction. Roughly speaking, cars are expected to move more regular in direction (because they move along the roads) than individuals, while individuals move more regular in speed than cars (because cars have a broad range of speed and they frequently stop due to various situations, \eg traffic lights).
And partially due to these as well as the relatively lower average sampling rate of dataset UCar, it has poorer compression ratios than datasets Geolife and Mopsi when using Euclidean distance metrics PED and SED, as shown in Figures 16, 17, 19 and 20 (corresponding to Figures 15, 16, 18 and 19 in the previous manuscript). However, when using the direction metric DAD (recall cars are regular in direction), dataset UCar does have better compression ratios than Geolife and Mopsi, as shown in Figures 18 and 21 (corresponding to Figures 17 and 20 of the previous manuscript).

To clarify this, we have revised the sentence ``The dataset collected by cars (e.g., UCar) also has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cares typically move more regularly than individuals'' to ``\textcolor{blue}{When using DAD,} the dataset collected by cars (e.g., UCar) has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cars typically move more regularly than individuals \textcolor{blue}{in directions}'' in the second paragraph of Section 6.3.1.
Thanks for pointing out these!


\textbf{[R1C12]} \emph{The impact of error bounds on compression ratios is straightforward. The bigger the error bounds, the better the compression ratio. However, the size of trajectory on the compression ratio is not that straightforward. Can the authors please comment on the observations we could make from Figures 18 -20? }

Yes, data size does not have significant impacts on compression ratios. (1) For an online or one-pass algorithm that is compressing a trajectory, once some distance is greater than the error bound, it is sure to output a line segment and continue to compress the subsequent data points. As a result, the data size has few impacts on the compression ratios of online and one-pass algorithms.
(2) For batch and the optimal algorithms, a long trajectory is connected by a number of sub-trajectories $[\dddot{T}_1,\dddot{T}_2,..., \dddot{T}_m, ]$ that order by time. As each sub-trajectory could be simplified to several relatively fixed line segments and sub-trajectories are relatively independent to each other, the data size also has few impacts on the compression ratios.


%Our tests reveal a similar result.
%More specifically,  Then suppose this original trajectory is being simplified by some online or one-pass algorithm that runs in an incremental manner, it is reasonable to conclude that the appending of $\dddot{T}_{m+1}$ might have some impacts of the representation (simplification) of $\dddot{T}_{m}$, but it's hard to have impacts on $\dddot{T}_i$ before $\dddot{T}_{m}$.

We have clarified this in the third paragraph of Section 6.3.1 (the 2{nd} item). Thanks!



\textbf{[R1C13]} \emph{ In Page 23 (lines 24 - 31), it is stated that ``given the same error bound $\epsilon$, the compression ratios of algorithms using PED are obviously better than using SED". This is actually misleading. Although the observation is that algorithms using PED could achieve higher compression ratios, the statement ignores the fact the PED does not consider the temporal dimension at all while SED does. The comparison is not fair at all! If the higher compression ratio is achieved by ignoring the error in the temporal dimension, it has to be stated clearly.}



Recall that different applications may have different requirements, and the purpose of this evaluation is exactly to
provide guidelines on how to choose appropriate algorithms and distance metrics for practical applications.
That is, this evaluation aims to summarize the state-of-the-art of the error bounded trajectory simplification algorithms and distance metrics, analyze the features of them and highlight the advantages and limitations of each kind of technologies, and also conduct a systematic experimental study under uniform quality criteria (including compression ratios, errors, running time, aging friendliness and query friendliness) for large-scale trajectory data, so as to provide a full picture of line simplification algorithms and distance metrics to users.

(1) We have summarized the major characters of algorithms and distance metrics, and highlight the advantages and limitations of distance metrics at the beginning of the article (please refer to the ``algorithm taxonomy'' and ``distance metrics" parts of Section 1), with respect to your advice of [R1C5]. And yes, PED largely treats a trajectory as a sequence of spatial data points, thus, it has better compression ratios than SED at a price of losing temporal information. This is also explained in Section 1,

(2) We have objectively evaluated these technologies under the uniform quality criteria (including compression ratios, errors, running time, aging friendliness and query friendliness), and the same setting and parameters. {\em Following the common practice in this field}, each quality metric is tested separately. These tests and comparisons are fair, especially under the premise that we have claimed the features, advantages and limitations of those technologies. %, as shown in (1), and

(3) We agree that the higher compression ratio is not always the better and a single measure is not enough to determine an algorithm or distance metric, \ie \emph{users need to consider multiple metrics to choose a correct algorithm and distance metric} so as to satisfy their unique requirements (this is also your advice of [R1C6]). In other words, users need to first choose a proper quality metric according to the application requirements, then choose an appropriate algorithm \wrt the quality metric.

Accordingly, we have clarified this in the penultimate paragraph of Section 6.3.1. Please also refer to the responses to [R1C5] and [R1C6] for more details. Thanks!

\textbf{[R1C14]} \emph{ In Page 27 (lines 44 - 45), it is stated that ``When using DAD, the running time from the smallest to the largest is one-pass algorithms Intersect and Interval, batch algorithms TP and DP, and online algorithm OPW." However, algorithms actually perform slightly different at various datasets. It's not accurate to state that online algorithm OPW is always the worst, as it actually performs much better than TP and DP in the dataset Geolife (as shown in Figure 32(2)).}

Yes, OPW performs much better than TP and DP in Geolife with the error bound fixed to $45$ degrees, as shown in Figure 32 (33 in this new submission), and it also performs better than DP in UCar and Mopsi when the error bound is small, \eg it's less than 30 degrees, as shown in Figure 29 (30 in this new submission).

We have revised the statement, and this sentence is replaced by ``When using DAD, one-pass algorithms Intersect and Interval run prominently faster than batch algorithms TP and DP and online algorithm OPW''.
Thanks for pointing out this!

\textbf{[R1C15]} \emph{ When evaluating the running time of different algorithms under various error bounds, different algorithms demonstrate different trends. The authors might want to explain why algorithms change the trends in certain ways (e.g., why error bounds do not have any impact on the running time of SIPED, why TP, OPW, BQS incur longer running time as error bounds increase, why ......).}

Yes, different algorithms demonstrate different trends under various error bounds as they have different routines and principles for trajectory simplification. (1)  For batch algorithms, the running time of DP and TP decreases or increases with the increase of error bound $\epsilon$, respectively, due to the top-down and bottom-up approaches that they apply.
%
(2) For online algorithms, BQS and OPW both have poor efficiency as they finally need batch approaches to simplify buffered data. SQUISH-E is faster than BQS and OPW at a cost of poorer compression ratios.
%
(3) For one-pass algorithms, SIPED, OPERB, CISED, Interval and Intersect show a linear running time that is consistent with their time complexity analyses. They are not very sensitive to error bound $\epsilon$ and also scale well with the increase of trajectory size on all datasets as a data point is processed only one time during the whole process.

We have provided a detailed analysis of these trends, summarized in the ``analyses of LS algorithms" part of Section 6.3.3.
%
We believe it is helpful for users to understand the characters of these algorithms, and to choose an appropriate algorithm to meet their needs.

\textbf{[R1C16a]} \emph{ Figures 33 - 38 report the average errors of where\_at queries based on trajectories compressed using different line simplification algorithms. The title of 'Evaluation of spatio-temporal queries' is misleading as only one type of queries is considered.}

Yes, we have revised the title of ''Evaluation of spatio-temporal queries" to ``Evaluation of \emph{where\_at} queries". Thanks!

\textbf{[R1C16b]} \emph{In addition, it is necessary to consider \emph{when\_at} query as it is a critical building block for many applications too.}

Yes, we have added tests on the \emph{``when\_at"} query in Section 6.3.5. Thanks for the suggestion!

\textbf{[R1C17]} \emph{ Based on the results reported in Figures 33 -38, PED and DAD are actually not proper distance metrics that should be considered when we apply line simplification to compress the trajectories and meanwhile want to preserve the utility of the compressed trajectories. Especially, Figure 21 (avg PED errors under different error bounds) and Figure 33 (avg PED query errors under different error bounds) have different trends (Figure 23 and Figure 35 have different trends too), which further demonstrates that PED and DAD are actually misleading. An algorithm that can achieve good performance in terms of PED/DAD might not be able to guarantee its performance in real applications. }

Yes, a line simplification algorithm using PED or DAD is not able to guarantee the error bounds of spatio-temporal queries, which makes it inappropriate in scenarios that spatio-temporal queries are required. However, they still have their usages, \eg in some trajectory clustering that cares about the shape of a trajectory rather than the detailed positions of an individual. More detailed discussion about distance metrics please refer to [R1C5].

In practice, users first choose the key quality metrics according to the application requirements, then choose an appropriate algorithm according to those metrics.
Accordingly, we have explained the quality metrics (including compression ratios, errors, running time, aging friendliness and query friendliness) in Section 1, and evaluated line simplification technologies under the uniform quality criteria in Section 6. Our work provides a whole image of line simplification algorithms, such that {users are able to choose correct algorithms} \wrt multiple quality metrics satisfying their unique requirements (the quality criteria problem is also your great advice in [R1C6]).

We have highlighted this in the penultimate paragraph of Section 6.3.6.
Thanks!

\textbf{[R1C18]} \emph{ This paper focuses on applying line simplification for trajectory compression. Given the fact that there are different ways to compress trajectories (e.g., [1], [2]), it might be very helpful to give a brief introduction on different trajectory compression techniques, which could help audience to understand the pros/cons of applying line simplification to compress trajectories. }

\emph{ [1] CiNCT: Compression and Retrieval for Massive Vehicular Trajectories via Relative Movement Labeling. Satoshi Koide, Yukihiro Tadokoro, Chuan Xiao, Yoshiharu Ishikawa. In ICDE, 2018.}

\emph{ [2] COMPRESS: A Comprehensive Framework of Trajectory Compression in Road Networks. Yunheng Han, Weiwei Sun, Baihua Zheng. TODS 42, 2, 11:1--11:49, 2017.}


{Yes, we have added an introduction to the semantic-based methods (including methods based on road networks) in the appendix. Thanks!}
%The relationship between them and line simplification is also discussed.

%******************* reviewer 2 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 2.}

\line(1,0){100}

\textbf{[R2D1]} \emph{In page 2, the authors mention that ``important aspects of trajectory simplification (compression ratios, running time and aging friendliness) are not systematically studied'' for [41]. Not sure which ``important aspects'' the authors refer to, as I found running time and the trade-off between compression ratios and errors have been reported in [41].}

(1) Running time: [41] ([61] in this revised version) does show a table listing \emph{``compression time per trajectory point''} of each algorithm.  However, the efficiency of these algorithms \wrt error bounds and trajectory sizes is not reported. Further, those algorithms are not programmed in the same language - this is somehow unfair for comparing their efficiency.

(2) Compression ratios: [41] ([61] in this revised version)  does report the relationship between compression ratios and errors. However, both the compression ratios, the average errors and trajectory sizes are not separately tested \wrt error bounds.

(3) Aging friendliness: the data aging problem is not covered in [41].

(4) Some important algorithms, \eg CISED and SIPED, are not covered in [41] (CISED is published after [41]).

We have revised the motivation part of the manuscript to clarify these.
Thanks for pointing out this!


%Indeed, our study is a necessary complement to existing studies by providing a systematic evaluation and analyses of error bounded trajectory simplification algorithms.
%Comparing with [41], this work has the following new findings:
%(1) compression ratios and efficiency of optimal, batch, online and one-pass algorithms are systematically evaluated \wrt distance metrics (PED, SED and DAD), error bounds and data sizes, as summarized in the Section 6.3.6,
%(2) aging friendliness and safety are comprehensively evaluated, as reported in Section 6.3.4,
%(3) though online algorithm DOTS shows better trade-off between compression ratio and average error [41], it is not effective in terms of compression ratios. Besides its Java implementation is not efficient in large datasets, and
%(4) one-pass algorithm SIPED ($\epsilon$) and one-pass algorithm CISED, either CISED($\epsilon$) or CISED-W, are efficient as well as has good compression ratios.

\textbf{[R2D2]} \emph{In page 3: $\epsilon_1$ and $\epsilon_2$ appeared without explanation.}

Yes, we have fixed it, {where $\epsilon_1$ and $\epsilon_2$ are the error bounds set in the first and second times of simplification, respectively}.
Thanks!

\textbf{[R2D3]} \emph{ In page 4: It's better to move the comparison with [41] to the motivation part, in order to assure the necessity of a new empirical study.}

Yes, we have moved the comparisons with [41] to the motivation part. Thanks for your advice!
%say, the very recent study [41] does evaluate a wide range of trajectory simplification algorithms.
%However, it provides {an experimental study} on compression errors and spatio-temporal query analyses only, and important aspects of trajectory simplification (compression ratios, running time, aging friendliness and safety) are not systematically studied.
%{More specifically,}
%{(1) it has a limited evaluation on running time: it reports the \emph{compression time per trajectory point} of each algorithm on full datasets, however, an algorithm has different running time \wrt error bounds and the running time of many algorithms are not linear to the size of a trajectory (thus it may cost different time on one point when the input trajectories have different sizes), meaning the impacts of both the sizes of trajectories and the error bounds on the running time should be tested;}
%
%{(2) it has a limited evaluation on compression ratios: it does report the relationship between compression ratios and average errors, but the impacts of the error bounds on the compression ratios are not separately tested; and }
%
%(3) it does not cover the \emph{data aging} problem.
%
%{Besides, some important algorithms, \eg optimal algorithms using PED and SED and one-pass algorithms SIPED and CISED, are not investigated in [41].
%



%that is helpful to elaborate the motivation of the work

\textbf{[R2D4]} \emph{The authors mention that one of the main contributions is to re-implement the methods with Java. So compared with the running time results reported in [41], any new or different findings are revealed?}


Yes, compared with [41], we have found different results \wrt running time.

In [41], the running times of algorithms from the fastest to the slowest are Intersect, Interval, \textcolor{blue}{DP}, OPERB, \textcolor{blue}{DOTS}, \textcolor{blue}{SQUISH-E} and BQS, \eg they need 0.28, 0.29, \textcolor{blue}{0.46}, 0.9, \textcolor{blue}{1.58}, \textcolor{blue}{26.78} and 729.49 microseconds respectively to process one data point of the Geolife dataset.
However, in our tests (re-implement the methods with a uniform language, Java), the running time from the fastest to the slowest is typically Intersect, Interval, OPERB, \textcolor{blue}{SQUISH-E}, \textcolor{blue}{DP},  \textcolor{blue}{DOTS} and BQS in all datasets, \eg they need 0.59, 1.08, 1.09, \textcolor{blue}{3.44}, \textcolor{blue}{15.72}, \textcolor{blue}{45.71} and 59.42 microseconds respectively to process one data point of the Mopsi dataset (this example data is collected from dataset Mopsi with error bound of 40 meters or 45 degrees. When processing one point, such an algorithm has the similar running time in Mopsi compared with Geolife, as shown in Figures 28-33). In our tests, DP (15.72) and DOTS (45.71) are much slower, while SQUISH-E (3.44) is much faster, than those (\textcolor{blue}{0.46}, \textcolor{blue}{1.58}, \textcolor{blue}{26.78}) reported in [41].


Moreover, our tests also reveal that the findings of the running time of algorithms with respect to the length of a trajectory (counted in the number of data points), the type of distance metrics and the variation of error bounds, that are not systematically studied in [41].



We have clarified this in Section 6.3.6, say, ``our tests tell that the running time from the fastest to the slowest is normally Intersect, Interval, OPERB, SQUISH-E, DP, OPW, DOTS and BQS in all datasets, which is partially different from [41] that is Intersect, Interval, DP, OPERB, DOTS, OPW, SQUISH-E and BQS". Thanks!


%First of all, we re-implement the methods with the same language, Java, for a fair comparison.


\textbf{[R2D5]} \emph{In Table 1, what are the criteria for the selection of ``representative'' algorithms? The authors may need to explicitly explain this in this paper.}

We consider the performance of algorithms, the supporting of distance metrics and the novelty of their key ideas to select the representative algorithms. More specifically,
(1) for optimal algorithms, {because all the optimized optimal algorithms have the same effectiveness using the same distance metric, and algorithm Optimal supports all three distances PED, SED and DAD while others are not, we choose it as the representative of the optimal line simplification algorithms,}
%
(2) for batch algorithms, {because DP is the typical top-down algorithm and TP is the typical bottom-up algorithm, and they both support PED, SED and DAD, and we choose  both of them as the representatives of batch algorithms,}
%
(3) for online algorithms, because algorithms BQS, SQUISH-E and DOTS are the state-of-the art of online algorithms, in particular, BQS is the performance optimized online algorithm specific for PED, SQUISH-E is the famous online algorithm specific for SED, and {DOTS is the online algorithm using LISSED and is recommended in the recent evaluation work [41]}, we choose them as the representatives of online algorithms using PED, SED and {LISSED}, respectively. Besides, because no specific online algorithms have been developed for DAD, we alternatively choose OPW, a well-known online algorithm based on DP that is compatible with DAD, as the representative of online algorithm using DAD, such that each distance metric has at least a representative algorithm, and
%
(4) for one-pass algorithms, because SIPED, OPERB, CISED, Intersect and Interval are the state-of-the-art of this kind of algorithms, in particular, algorithms OPERB and SIPED are specific to PED, CISED is specific to SED, and Intersect and Interval are specific to DAD. We choose them as the representatives of one-pass algorithms.

To clarify these, we have added a brief explanation of the criteria after Table 1, and explained the details selections in the top paragraphs of Sections 3, 4.1, 4.2 and 4.3.
%Thanks!

\textbf{[R2D6]} \emph{Obviously, data aging is a very important contribution in this paper, because it was not examined in [41]. However, the situation becomes very awkward when most of the existing algorithms are not data aging friendly. Among all the methods in Table 1, only DP (with SED and PED as the distance measure) is data aging friendly. (1) {If a user considers data aging is important, then he/she has no choice but to use DP}. (2) {The authors may need to provide more convincing arguments for the necessity of examining data aging}.}

We should first explain better the \emph{data aging problem} and the \emph{aging friendliness property} of line simplification.

As stated in [Cao 2006], the older the information gets, the less precision may be necessary. Thus it is possible that the coarseness of the trajectory approximation is allowed to increase as time progresses. In this context, the \emph{data aging problem} is of obtaining a lower level of precision from a higher one [Cao 2006], more specifically, \emph{what is the right way to get the coarser trajectory? What line simplification algorithms should we use in the first and second rounds of simplifications? If the first round of simplification uses algorithm $\mathcal{A}_1$, can we use algorithm $\mathcal{A}_2$ in the second round of simplification? How to set the parameter of error bounds in these simplifications? And after multiple times of simplifications, does the coarse trajectory still have bounded errors \wrt the original trajectory?} These problems are initiated by and partially solved in [Cao 2006] that focuses on the optimal algorithms and the DP algorithm in data aging.
%The \emph{data aging} problem is rather an important requirement derived from the management of trajectory data;
Then, the \emph{aging friendliness property} of simplification algorithms is,  given a simplification algorithm $\mathcal{A}$ and a distance function/metric $\mathcal{M}$, algorithm $\mathcal{A}$ is \emph{aging friendly} with respect to  $\mathcal{M}$ if for every $\epsilon_1$ and every $\epsilon_2$ such that $\epsilon_1 <\epsilon_2$, and
for every trajectory $T$ , $S_A(T, \epsilon_2,\mathcal{M}) = S_A(S_A(T, \epsilon_1,\mathcal{M}), \epsilon_2, \mathcal{M})$ [Cao 2006]. [Cao 2006] also proves that algorithm DP is aging friendly.

We next answer the two questions that you raised above.



(1) \underline{``If a user considers data aging is important, then he/she has no choice but to use DP"}: This argue is somehow inaccurate. Indeed, yes, data aging is important. However, users do not necessary use aging friendly algorithms (DP) in all cases. The reason is,
\emph{data aging} is rather an important requirement derived from the management of trajectory data that cares about the broad problems related to ``obtaining a lower level of precision from a higher one" as mentioned above, while aging friendliness is just a special feature of some algorithms that satisfy or not. For example, in practical, the raw trajectory may be simplified by some algorithm other than DP and then uploaded to data server, or the data is first simplified by DP but due to the large data size and for the efficiency consideration, users do not want to re-compress these trajectories by DP. In these cases, can users use any other algorithm in the second round of simplification? How to set the parameter of error bounds in these simplifications? And after multiple times of simplifications, do the coarse trajectories still have bounded errors \wrt the original trajectory? These questions all belong to the data aging problem while they are beyond the scope of the aging friendliness.

In short, data aging is a broad problem, while aging friendliness is a property of some algorithms in data aging, they are not equivalent. Thus, when we talk about data aging, it does not mean we have to choose the \emph{aging friendly} algorithm (DP) to compress trajectories --- though given the same error bounds, it may bring better compression ratios in data aging; and sometimes, even we choose DP in the second round of simplification, the aging friendly property may not be promised in the case that the first round uses algorithms other than DP, or the start and end points are changed due to the increase of data sizes.
%Also notice that compression ratios is only one consideration on choosing an algorithm, say, there are also errors and efficiency that impact the selection of a line simplification algorithm.
%That is to say, any algorithm could be used in data aging.

(2) \underline{``The authors may need to provide more convincing arguments for the necessity of examining data aging"}: We have mainly proved two important things \wrt data aging, (a) most algorithms are not \emph{aging friendly} (Section 5.1), and (b) all algorithms are error bounded in data aging (Section 5.2), \ie let $\mathcal{A}$ be a line simplification algorithm,  $\mathcal{M}$ be a distance metric, and $\epsilon_1>0$ and $\epsilon_2>0$ be error bounds, the error between the original trajectory $\dddot{\mathcal{T}}$ and the simplified trajectory $\overline{\mathcal{T}}=\mathcal{A}(\mathcal{A}(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{M}), \epsilon_2, \mathcal{M})$ is not more than $\epsilon_1+ \epsilon_2$. Which means that, in data aging, \emph{one can freely re-compress trajectories by any algorithm as long as he uses the same distance metric.} The aging friendliness and the bounded errors of algorithms (Table 3) are two important findings of our study, these together provide a full picture of the data aging problem, and provide users on how to choose an algorithm and set the parameter of the error bound in data aging.
%, and also serves as the reason of examining data aging

We have clarified these in the first paragraph of Section 5, and the last paragraphs of Sections 5.1 and 5.2. Thanks for pointing out this!

[Cao 2006] Cao, H., Wolfson, O., and Trajcevski, G. Spatio-temporal data reduction with deterministic error bounds. VLDBJ 15, 3 (2006), 211--228.


\textbf{[R2D7]} \emph{In this paper, only three types of distance measure PED, SED and DAD are presented and compared. The other distance measures are briefly touched in the Appendix. Question here is that method like Dots is recommended for online compression in [41] as it shows better trade-off in terms of compression ratio and error. So when recommending users with suitable compression algorithms, would the guidance be biased if the authors exclude certain existing methods?}

Yes, we have included the local integral square SED (LISSED). %and evaluated algorithms OptLISSED, MRPA, DOTS and OLTS using LISSED in the revised version.
In particular, we have (1) briefly introduced LISSED in Section 2 after the definition of the Synchronous Euclidean Distance (SED) and algorithms OptLISSED, MRPA and DOTS using LISSED in Section 4,   (2) chosen DOTS as the representative of these algorithms and described it in Section 4.2.4, and (3) tested and reported our findings on DOTS and LISSED in Section 6.

Thanks for your advice!

%Also note that, in this revision, {the LISSED is rather a special SED-based ``error measure" or a methodology that efficiently calculate SED errors than a kind of ``distance measure"}.

\textbf{[R2D8]} \emph{In page 23: the authors mentioned that ``in practice, SED has obviously better compression ratios than DAD..''. This is confusing to me because the distance unit for SED is meter and unit for DAD is degree. How can they be compared? It seems like the effect of epsilon=100m in SED is similar to epsilon=60 degree in DAD?}

As you have mentioned, SED is a Euclidean distance metric, having a value in $[0, \infty]$, and DAD is a direction metric, having a value in $[0, 360]$ degrees. Hence, it is hard to compare them under absolutely fair conditions. However, we believe it is helpful to give readers an intuitive impression on the performance of DAD compared with SED. Thus, we alternatively consider two practical scenarios: one uses SED with $\epsilon  \le  100$ meters and the other uses DAD with $\epsilon \le 60$ degrees, and then we compare the performance of them, \eg the performance of SED with $\epsilon=100/k$ meters vs. that of DAD with $\epsilon=60/k$ degrees ($k\ge 1$).


{We have revised the last paragraph of Section 6.3.1 to clarify this. }
%Thanks!

\textbf{[R2D9]} \emph{Besides clarifying the differences with [41] in the introduction, it is better to summarize the differences of experimental findings at the end of the paper, e.g., from the experimental results, what are the new insights not covered in [41] and what are the different/conflicting findings?}

Comparing with the recent experimental study [41], this work has the following new findings:
(1) the compression ratios and average errors of optimal, batch, online and one-pass algorithms \wrt distance metrics (PED, SED and DAD), error bounds and data sizes are revealed, as summarized in Section 6.3.6,
(2) the efficiency of algorithms \wrt distance metrics, error bounds and data sizes are also revealed, as summarized Section 6.3.6. Besides, our tests tell that the running times from the fastest to the slowest are normally Intersect, Interval, OPERB, SQUISH-E, DP, DOTS and BQS in all datasets, which is partially different from [41] that are Intersect, Interval, DP, OPERB, DOTS, SQUISH-E and BQS,
(3) aging friendliness and errors are revealed, as reported in Section 6.3.4,
(4) though online algorithm DOTS shows a better trade-off between compression ratios and errors [41], it is not effective in terms of compression ratios,
(5) one-pass algorithm SIPED ($\epsilon$) is efficient and has good compression ratios, and
(6) one-pass algorithm CISED, either CISED($\epsilon$) or CISED-W, is better than batch and online algorithms (online algorithm DOTS~is recommended in [41]) in terms of compression ratios and efficiency.

We have summarized these at the end of Section 6.3.6. Thanks for pointing out this!


%******************* reviewer 3 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 3.}

\line(1,0){100}

\textbf{[R3C1a]} \emph{
Sec. 4.1. The min-\# optimal version from [4] has different complexities depending on whether the polyline is open or closed (as well as convex vs. concave). The authors have not properly brought this in context.}

Yes, [4] proves the \emph{min-$\#$} problem (using PED) for a general polygonal curve can be solved in $O(n^2)$ time by using the \textit{sector intersection} mechanism, and for some special curve, either open or closed (if there is an edge joining the first and the last points, then it is closed, and is open, otherwise) polygonal curve that forms  a part of a convex polygon, this problem can be solved in $O(n)$ time. In general, a trajectory is not necessary a convex or closed polygonal curve, instead, it is often open and concave. Thus, the best optimal algorithm for trajectory simplification using PED still has $O(n^2)$ time.

We have revised the second paragraph of section 3 to clarify this. Thanks for pointing out this!

\textbf{[R3C1b]} \emph{Also, in multiple places, the authors are bringing the notion of the ``3D" - however, they do not mention the reference [r1] which was the first one to extend the algorithm in [4] to 3D. }

\emph{[r1] Gill Barequet, Danny Z. Chen, Ovidiu Daescu, Michael T. Goodrich, and Jack Snoeyink. Efficiently approximating polygonal paths in three and higher dimensions. Algorithmica, 33(2):150--167, 2002.}

Yes, in computer graphics and cartography, there is another 3D space, \ie \emph{x-y-z} 3D space, where {\em z} is height, and some works solve the \emph{min-\#} problem in the \emph{x-y-z} 3D space. Among them, authors in [r1] also extend the \textit{sector intersection} method to the \textit{off-line ball-inclusion testing} in the \emph{x-y-z} 3D space, such that they are able to develop efficient near-quadratic-time algorithms in three dimensions for solving the \emph{min-\#} and \emph{min-$\epsilon$} problems. %

We have clarified this in the first paragraph of Section 4.3.3. Thanks!

\textbf{[R3C2]} \emph{
Sec. 4.2. One aspect that is context-dependent (and often implicitly ignored) in online algorithms, is the communication overhead (which, in turn, may imply energy consumption in sensor networks); along with the complementary aspect of ``freshness" of the compressed data in the respective sink. Please see [r2] below.}

\emph{[r2] Oliviu Ghica, Goce Trajcevski, Ouri Wolfson, Ugo Buy, Peter Scheuermann, Fan Zhou, Dennis Vaccaro: Trajectory Data Reduction in Wireless Sensor Networks. IJNGC 1(1) (2010)
}

{Yes, in Wireless Sensor Networks, online algorithms should address the problem of balancing the trade-off between the energy cost due to communication and the accuracy of the trajectories' detection and representation [r2].} Besides, the ``freshness'' (i.e., the latency) of the simplified data in the sink and/or data server could be controlled by a careful management of the data buffer of an online algorithm [r2]. The freshness of simplified data is related to the buffer size of the simplification algorithm, that is, the smaller the buffer size, the fresher the output data. It is also related to the efficiency of the algorithm, that is, if the algorithm needs a shorter time to compress (\eg a one-pass algorithm), then the result data must be fresher.

We have clarified this, with respect to [r2], in the first paragraph of Section 4.2.
Thanks!

\textbf{[R3C3a]} \emph{
Sec. 4.3. Actually [13] does provide a kind of a taxonomy, and it would be nice to compare it with the present article. }

Indeed, our taxonomy (batch, online and one-pass sub-optimal line simplification algorithms) is equivalent to [13] (respectively, offline, online and real-time algorithms).
We prefer the words ``batch" and ``one-pass" in that ``batch'' is a traditional name in trajectory compression and ``one-pass'' catches  better the common feature of the applied \emph{distance checking policies} of those algorithms.

We have clarified this in the 3rd paragraph of Section 1 (algorithm taxonomy).
Thanks for your advice!

\textbf{[R3C3b]} \emph{Also, [37] has a claim/proof that if the online version uses $\varepsilon$ as an error-bound in the LDR, then the final outcome is an equivalent to a compression of $2 \cdot \varepsilon$ applied to the entire historic trajectory (if it were available). This, in a sense, is a form of a justification for the use of $\varepsilon$/2 in works like CISED ([14]).}

{Yes, [37] proves that if LDR uses $\epsilon/2$ as the error-bound in position tracking, then its output trajectory has a max error not greater than $\epsilon$ to the original trajectory. Thus, it is a trajectory simplification algorithm as well as a position tracking protocol.} Besides, its idea of half $\epsilon$ is earlier than CISED and Intersect.

We have clarified this in the 3rd and 5th paragraphs of Section 4.3. Thanks!
%Thanks for pointing out this!

\textbf{[R3C4]} \emph{
In several places, the authors are making statements that are not properly phrased, and some that can be taken out of context. Examples:}

\emph{1. The authors should try to define terms before their first use. Example: on p.1 in the Introduction, the ``min-\#" is brought up, without properly explaining its meaning.}


\emph{2. p.2: ``...no need of extra knowledge and suitable for freely moving objects [28]...". A statement of this sort does not do justice to [28].
Namely, [28] considers what happens with compression if the trajectories are constrained to move along road networks - but, it also is based on the motivation that in such settings there may be a different perspective - i.e., the overall-compression of the DB.}


\emph{3. p.33: the authors make a claim to the effect of: ``These methods lack the capability of compressing..." when describing the min-$\epsilon$ variant. The statement needs to be clarified, or put in the more appropriate contexts: the min-$\epsilon$  is a complementary one to min-\# and, as such, it never makes any other claim that it will yield the minimum error, given a fixed ``budget" of m points. However, that does not mean that this variant of the problem, and the algorithmic solution, doesn't have its own merits.}



(1) Yes, the \emph{min-\#} problem of trajectory simplification has now been explained in Section 1 (the second paragraph).

(2) Yes, we have replaced the sentence to ``light and applicable to resource-constrained devices'' in Section 1 (the second paragraph).

(3) Yes, we have revised these sentences to ``the \emph{min-$\epsilon$} problem: given $m$, constructs an approximate curve consisting of at most $m$ line segments with the minimum error. Algorithms like SQUISH ($\lambda$) and SQUISH-E ($\lambda$) solve this problem and are the complementary ones to those solving the \emph{min-$\#$} problem." Besides, the introduction to the \emph{min-$\epsilon$} problem is moved to the second paragraph of Section 1.

Thanks for your advice!

\line(1,0){500}



Your sincerely,

Anonymous authors
%Xuelian Lin, Shuai Ma, Yanchen Hou, Yihao Fu,  and Tianyu Wo

%\bibliographystyle{abbrv}
%\bibliography{sec-ref}


\end{document}
