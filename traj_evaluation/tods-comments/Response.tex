\documentclass{letter}
\usepackage{geometry}

% duan
\usepackage{xspace}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{multirow,amsmath, array,colortbl}


\newcommand{\marked}[1]{\textcolor{red}{#1}}

\newcommand{\kw}[1]{{\ensuremath {\mathsf{#1}}}\xspace}

\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}
\newcommand{\kwlog}{\emph{w.l.o.g.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\sstab}{\rule{0pt}{8pt}\\[-2.4ex]}

\newcommand{\topk}[1]{\kw{top}--\kw{#1}}
\newcommand{\topdown}{\kw{topDown}}
\newcommand{\extsubgraph}{\kw{compADS^+}}
\newcommand{\drfds}{\kw{FIDES^+}}
\newcommand{\extsubgraphold}{\kw{compADS}}
\newcommand{\findtimax}{\kw{maxTInterval}}
\newcommand{\findtimin}{\kw{minTInterval}}
\newcommand{\meden}{\kw{MEDEN}}

\newcommand{\tranformgraph}{\kw{convertAG}}
\newcommand{\mergecc}{\kw{strongMerging}}
\newcommand{\strongpruning}{\kw{strongPruning}}
\newcommand{\boundedprobing}{\kw{boundedProbing}}

\newcommand{\AFPR}{\kw{AFP}-\kw{reduction}}
\newcommand{\nwm}{{\sc nwm}\xspace}


\newcommand{\cone}[1]{{$\mathcal{C}{#1}$}}
\renewcommand{\circle}[1]{{$\mathcal{O}{#1}$}}
\newcommand{\pcircle}[1]{{$\mathcal{O}^c{#1}$}}

\newcommand{\vv}{\overrightarrow}


\newcommand{\todo}[1]{\textcolor{red}{Todo...#1}}
\begin{document}





Prof. {Chris Jermaine} \\
Editor-in-Chief		\\
ACM TODS	\\



Dear Prof. Jermaine,

Attached please find a revised version of our submission to
the TODS, \emph{Error Bounded Line Simplification Algorithms for Trajectory Compression: An Experimental Evaluation}.


%%%{\textbf{[Comments from Editors]}.\emph{These reviews, by recognized experts in the field, have obviously been prepared with care. Based on the reviews, the recommendation by handling Associate Editor Dr. Seeger, and my own assessment, I find that the paper needs to undergo a successful major revision to be acceptable for publication in TODS. As Dr. Seeger wrote to me, the reviews are quite consistent: two of them ask for a major revision, one for a minor revision. All of them agree that the survey paper is well written. Reviewer 1 rises most concerns regarding the variety of methods considered (the paper does not consider techniques where new points are used for compression), not considering map matching as a preprocessing step, error metrics, impact on applications, and the small data sets used in the experiments. The authors should carefully consider these points.}}


The paper has been substantially revised according to the comments of you, Dr. Seeger and referees. In particular, we have added{
	(a) techniques that allow data interpolation (namely, weak simplification), \eg~\kw{CISED}-{W},
	(b) techniques that use the Local Integral Square SED (LISSED) as error measure,  \eg~\kw{DOTS},
	(c) experimental studies on an additional application named ``when\_at" query, 
	(d) a larger dataset of UCar in the experimental studies, and
	(e) a brief introduction to map-matching-based trajectory compression methods in appendix.}
Besides, an explanation on ``why map-matching is not considered as a pre-processing step of trajectory simplification" is provided in {[R1C3]}.


We would like to thank you, Dr. Seeger and all the referees for your earnestness and conscientiousness, and for your valuable comments.

Below please find our detailed responses to the comments.



%******************* reviewer 1 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 1.}

\line(1,0){100}


\textbf{[R1C1]} \emph{What is ``min-\# problem" mentioned in Section 1 (second paragraph)?}


The \emph{$min-\#$ problem} of trajectory simplification is, given a curve (\ie an original trajectory) and an $\epsilon$, to construct an approximate curve with error within $\epsilon$ and, at the same time, having the minimum number of line segments.

We have explained it in Section 1 (the second paragraph), together with an explanation of the \emph{min-$\epsilon$} problem (which is the response to R3C4(3), that is, given $m$, construct an approximate curve consisting of at most $m$ line segments with the minimum error).
Thanks for pointing out this!

\textbf{[R1C2]} \emph{It is stated in the paper that ``The idea of piece-wise line simplification (LS) comes from computational geometry, whose target is to approximate a fine piece-wise linear curve with a coarse one (whose corresponding data points are a subset of the original one), such that the maximum distance of the former to the latter is bounded by a user specified threshold." This is actually misleading. Line simplification actually could be performed by using points other than original ones to represent the original line. The authors did mention those algorithms which consider points outside the original trajectories in the appendix. However, to make sure the correctness of the statements, it's better to mention that in the beginning of the paper. In addition, it is necessary to explain why this paper only considers the line simplification algorithms that consider the original points only. }

Yes, line simplification could use points other than original ones to represent the original line. Indeed, there are \emph{strong simplification} that outputs data points only belonging to the input trajectory, and \emph{weak simplification} that allows inserting new data points into the output trajectory.

(1) We have corrected the statements in the beginning of the paper (the second paragraph of Section 1), where ``(whose corresponding data points are a subset of the original one)" is replaced by ``(in computer graphics, it is basic that a planar line or curve is defined by a sequence of data points. If all data points of the coarse curve are a subset of the original curve, then it's a strong simplification; otherwise, it's a weak simplification)".

(2) Now, this paper has covered both strong and weak simplifications, and we have tested and analyzed ``weak simplification" algorithms (OPERB-A and CISED-W) in Section 6. 

Thanks for pointing out this!

\textbf{[R1C3]} \emph{For trajectories that capture the moving objects' movement along the road network, the points are normally mapped to road networks. In other words, it is not necessary to use piece-wise line representation. It looks like the authors have not considered this case, which is actually one of the most common cases. Can the authors please explain why map-matching step is not considered even for trajectories that capture the movements in road networks?}

%step 1, two kinds of compression methods, orthogonal to each other.
Yes, for a moving object in urban, its trajectory could be mapped onto the road network, and we have also observed that there is a kind of trajectory compression called road-aware trajectory compression [r1, r2] that matches data points onto roads and is used to compress trajectories for objects moving along the road network (we have introduced these works in Appendix ``Additional Trajectory Compression Algorithms"). The road-aware trajectory compression belongs to semantic-based trajectory compression, which is orthogonal to line simplification, \ie for trajectories that capture the moving objects' movement along the road network, they can be compressed by some road-aware trajectory compression method (including the map-matching step), or alternatively by some line simplification method (does not need the map-matching step). Both methods are technically applicable to this scenario. 

%For road-aware trajectory compression, it is able to represent a trajectory basing on long paths (a path is a sequence of roads connected one by one) and further mine the frequency patterns, so as to get the overall compression of trajectories; For line simplification, it is light ...

We next explain, in this evaluation, ``\emph{why map-matching step is not considered even for trajectories that capture the movements in road networks}?"
The reasons are two folds.
% Indeed, this question is related to ``why this paper focuses on line simplification and let alone map-matching". 

(1) line simplification is light and efficient, and is suitable to run on those resource-constrained end devices, on which map-matching is inefficient or even not applicable: A line representation of a trajectory is equivalent to a sequence of data points; and line simplification is light and easy to implement, and does not require extra information like road networks. These features together make it suitable to run on those resource-constrained end devices, which is a great advantage in practice. As we know, trajectory simplification is better performed as early as possible, \ie it's better to run trajectory simplification on mobile devices that collect GPS points, such that not only storage of data servers but also network bandwidth between mobile devices and data servers is saved. In this case, line simplification is a better choice than road-aware method as it is more suitable for those end devices, and map-matching is not necessary a step of the trajectory simplification (on end devices).

(2) it is a convenient, and somehow indispensable, way to directly represent the trajectory by line segments at the first (even on the sever-side): Moving objects often move in areas where no fine or up-to-date road network information is available, \eg the dataset Geolife includes some moving object alternately taking cars, trains and airplanes, and by feet, part of its data points are not on any road or they are on roads whose information is currently not available. In these cases, if we roughly match the trajectory onto road network at the beginning, then it might introduce errors caused by mis-map-matching. However, if we save the (simplified) line representation of the trajectory, then once the road network is available or updated, it can also conveniently be matched onto roads at the time we need. For the similar reason, some road-aware works have already applied this strategy, \eg the dilution-matching-encoding [r2] method, which first simplifies the trajectories by some line simplification method, then maps the simplified trajectories onto roads. 
%Anyway, it is an effective and even indispensable way to directly represent the trajectory by (simplified) line segments at the first (even on the sever-side).
%An line representation (original or simplified) of a trajectory can be matched onto road networks at anytime. 

Thus, even for trajectories that capture the movements in road networks, we will compress the trajectories by some line simplification method and do not consider the map-matching step (if needed, it could be the subsequent step of line simplification), and in this evaluation, we focus on line simplification and let alone map-matching.
 
We have clarified this in the second paragraph of Section 1, say, {``It is a great benefit that line simplification is suitable to run on resource-constrained end devices, such that the trajectories could be simplified at the early time, meaning not only the storage of data servers but also the network bandwidth between the end devices and the data servers is saved. Besides, line simplification could be combined with other technologies, like the dilution-matching-encoding method that maps the simplified trajectories to road networks, then mine and use high frequency patterns of compressed trajectories instead of roads, so as to further improve the effectiveness of trajectory compression."} Moreover, a discussion of additional trajectory compression methods (including the road-aware trajectory compression) are provided in the appendix.

%To clearify this, we have added a brief introduction to map-matching-based trajectory compression techniques in ``Appendix: Additional Trajectory Compression Algorithms". 


[r1] R. Song, W. Sun, B. Zheng, and Y. Zheng. Press: A novel framework of trajectory compression in road networks. PVLDB, 7(9):661--672, 2014.

[r2] R. Gotsman and Y. Kanza. A dilution-matching-encoding compaction of trajectories over road networks.
GeoInformatica, 2015.

%[r3] H. Elmeleegy, A. K. Elmagarmid, E. Cecchet and W. G. Aref. Online Piece-wise Linear Approximation of Numerical Streams with Precision Guarantees. PVLDB, 2009.


\textbf{[R1C4]} \emph{ In Page 6 (line 19), it is stated ``all points in the simplified trajectory belong to the original trajectory"? Why? If the error bound is the requirement, why we are not allowed to introduce new points, if it helps to improve the compression rate without violating the error bound defined? This is related to my comment C1. }

Yes, we have allowed interpolate new points into the output trajectories, and we have replaced the sentence ``all points in the simplified trajectory belong to the original trajectory" by ``{data interpolation is allowed." You can find it in the last paragraph of Section 2, also refer to [R1C2] for more details. 
	%Thank you!


\textbf{[R1C5]} \emph{The three distance metrics used in this paper are mainly for line simplification. However, if we apply line simplification to compress trajectories, some of the metrics are no longer that useful. For example, perpendicular distance metric (PED) does not consider temporal dimension, which means the error bounds based on PED consider spatial distances only. However, temporal dimension is very important to trajectories. I think it is necessary to revisit the metrics and highlight their limitations. Particularly, the error-bounds provided by these metrics could be misleading as the metrics do not fully reflect the loss of information. }

Yes, when researchers introduce PED to simplify trajectories, they initially treat the input trajectory as a sequence of spatial data points. As the result, \emph{line simplification algorithms using PED bring good compression ratios at a cost of losing temporal information of trajectories}. Hence, though it is error bounded by PED, it is not spatio-temporal \emph{query friendly}, \ie a spatio-temporal query like \emph{where\_at} on such a simplified trajectory possibly returns a point that has a distance great larger than the PED error bound used in the simplification. 
{In spite of that, PED still has its usages, \eg trajectory simplification using PED often serves as a pre-processing step of trajectory clustering that is the base of applications like traffic pattern recognition and urban planning, where rather the shapes of trajectories than the detailed positions of individuals are concerned.}

The SED of a point to a line segment is the Euclidean distance between the point and its \emph{synchronized point} \wrt the line segment, the expected position of the moving object on the line segment at the same time \emph{with an assumption that the object moves straightly along the line segment at a uniform speed}. {Most of these algorithms ensure that the \emph{maximal} SED from the output trajectory to the input trajectory is bounded by a preset SED error bound.} 
They friendly support applications such as spatio-temporal queries, \ie a spatio-temporal query like \emph{where\_at} on such a simplified trajectory will return the expected (synchronized) point that has a distance less than the error bound used in the simplification algorithm.
{Obviously, given the same error bound, algorithms using SED will output more points than PED as they also save temporal information.}

DAD is the direction deviation of the moving object, and 
the temporal information is also lost when using DAD, therefore, it is \emph{not spatio-temporal query friendly} too. It is important for applications such as direction-based trajectory clustering and query processing.

To clarify this, we have revised the descriptions of distance metrics PED, SED and DAD, and also highlighted their limitations, in the part of ``distance metrics", in Section 1. Thanks!

\textbf{[R1C6]} \emph{ The reason that we decide to store the trajectories (or the compressed version of raw trajectories to save storage space) is mainly to support some applications if required later, which might not require $100\%$ accuracy. For example, in order to find out the witness of a fatal car accident, police might want to locate all the drivers who pass by a specific location within a given time window by querying the trajectories. I am not very sure whether the error bounds based on different distance metrics are still valid for specific types of queries. {This survey does not consider the applications to be supported by the compressed trajectories at all. However, personally I think it is very important.} The authors did include one set of experiments towards the end of the paper. However, I think it is very important to explain the criteria of trajectory compression in the beginning of the paper, to offer the audience a complete image. In this paper, what are the key considerations when we compress trajectories? Is the compression ratio the only requirement? Is it necessary to support certain applications with error bounds? }

(1) The above example is indeed a case of spatio-tempporal \emph{intersect} query (also called a spatio-temporal \emph{range} query) [r4], defined in $intersect$ ($T$, \emph{Poly}, $t_1$, $t_2$), that returns \emph{true} if the trajectory $T$ intersects the polygon \emph{Poly} between the times $t_1$ and $t_2$. Authors in [r4] have proved that the SED-simplified trajectory is answer-error-bound for spatio-temporal queries (\emph{query friendly} in short) of \emph{where\_at, intersect} and \emph{nearest\_neighbor}, \ie if the simplification is SED error bounded by $\epsilon$, then the answer to such query is also error bounded by $\epsilon$.
However, if PED is used, then the answer to a spatio-temporal query on simplified trajectories is not error bounded [r4], \ie PED is not query friendly. Though DAD is not discussed in [r4] (DAD is developed after the publish of [r4]), it's obviously not spatio-temporal query friendly too. Our tests in Section 6.3.5 also support these statements.

(2) Yes, it's important to consider applications on the top of simplified trajectories. Thus, we have evaluated line simplification algorithms and distance metrics with respect to spatio-temporal queries \emph{where\_at} and \emph{when\_at}, two fundamental blocks of spatio-temporal queries. Note that [r4] have proved that ``the errors of the \emph{intersect} and \emph{nearest\_neighbor} query types are bounded if and only if the error of \emph{where\_at} is bounded", which indicates that the \emph{where\_at} query is one of the keys to test spatio-temporal queries; and \emph{when\_at} is another important functional block that is often discussed in comparing with \emph{where\_at}.
We have clarified this in Section 6.3.5.

(3) {Yes, it is important to explain the criteria of trajectory compression to offer the audience a complete image.} Thus, we have added a part of content named ``Quality criteria" in Section 1. Indeed, when we talk about the quality criteria of trajectory simplification, there are two levels, 
(a) the first level comes from trajectory simplification itself, including compression ratio, efficiency and errors of simplification. Most works in this area test their algorithms following a part or all of these criteria, and 
(b) the second level is from trajectory applications, \eg spatio-temporal queries, map-matching, trajectory clustering, anonymous and so on. Indeed, every trajectory application is applicable to evaluate the simplified trajectories from its point of view, \eg spatio-temporal queries concern the answer-errors and a map-matching or a trajectory clustering method concerns the accuracy.
%
In this paper, we follow the mainstream works in this area and test the compression ratios, efficiency and errors of simplification algorithms, and for trajectory applications, we choose spatio-temporal queries as the representatives as they are well-known and commonly used in the management of trajectories.
%Besides, as the data aging problem is a potential challenge to the management of trajectory, thus, the average and max errors of trajectory simplification algorithms as well as distance metrics in data aging are also tested.

We believe these are the key considerations when we compress trajectories. Besides, compression ratio is not the only requirement as mentioned above, and it is surely necessary to support certain applications with error bounds, \eg spatio-temporal queries of \emph{where\_at}, \emph{intersect} and \emph{nearest\_neighbor}.
Thanks for your great advice!

[r4] Cao, H., Wolfson, O., and Trajcevski, G. Spatio-temporal data reduction with deterministic error bounds. VLDBJ 15, 3 (2006), 211–228.

\textbf{[R1C7]} \emph{ The definition of Error Bounded Algorithms is not precise as the error bounds are not presented. }

We have revised the definition of error bounded algorithms in the penultimate paragraph of Section 2. That is, given a trajectory $\dddot{\mathcal{T}}\left[P_0, \dots, P_n\right]$ and a pre-specified bound $\epsilon$,
trajectory simplification algorithm $\mathcal{A}$ using PED (respectively, SED and DAD) is \emph{error bounded} by $\epsilon$ if for each point $P_k$ ($k\in[0,n]$) in $\dddot{\mathcal{T}}$, there exists a line segment $\mathcal{L}_i = \vv{P'_{s_i}P'_{e_i}}$ in $\overline{\mathcal{T}}$ with $s_i \le k \le e_i$ ($0\le i\le m$) such that the PED distance $ped\left(P_k, \mathcal{L}_i\right)$  (respectively the SED distance $sed\left(P_k, \mathcal{L}_i\right)$ and the DAD distance $dad\left(\vv{P_{k}P_{k+1}}, \mathcal{L}_i\right)$) is no more than  $\epsilon$.

Thanks for pointing out this!


\textbf{[R1C8]} \emph{ Fig.2 in Page 7 gives an example of reachability graph. However, that graph is incomplete. For example, there is no link between $P_0$ and $P_5$ but they are reachable. This is very misleading. }

Yes, to avoid the misleading, in Figures 1-3, 5-8 and 10, we have moved up point $P_4$ such that the PED distances from $P_4$ to line segments $\overline{P_0P_5}$, $\overline{P_1P_5}$, $\overline{P_2P_5}$ and $\overline{P_3P_5}$ are all larger than the error bound. Besides, all running examples have been revised.
Thanks for your seriousness and meticulousness!

%\begin{center}
%	%\includegraphics[width=1.\textwidth]{example-image}
%	\includegraphics[scale=0.75]{reachgraph.png}
%	\label{fig:rgraph}
%\end{center}

%As shown below, if we connect $P_0$ and $P_5$, then the \kw{PED} from $P_4$ to line segment $\overline{P_0P_5}$, \ie $|P_4P'_4|$, is a bit larger than the error bound $\epsilon$ (exact two frames in the figure). Thus, $P_5$ is not reachable from $P_0$, and there should be no link between them. 




\textbf{[R1C9]} \emph{ The datasets used in the experimental study were rather small. With 20M+ points, there is no need to compress them. Much larger datasets will be more realistic for the topic of trajectory compression.  }

Yes, we have extended dataset UCar from 200 to 1000 trajectories such that it totally has more than {100M} points. Besides, (1) we have already used the full datasets of Geolife and Mopsi, which are widely used in many recent trajectory compression works, \eg Geolife is used in the recent algorithm algorithm CISED [r5] and experimental study [r6], and Mopsi is used in [r5], and (2) \emph{the computing complexities of a line simplification algorithm are tightly related to the length (number of points) of the input trajectory rather than the number of trajectories}, and our datasets also include some long trajectories that each has more than {one million} data points which is quite memory consuming as the whole trajectory must be loaded in memory before the optimal and batch algorithms start to run, and time consuming as the optimal and batch algorithms each need {days} to run the whole tests listed in Section 6. %Thus, an even larger datasets is quite not friendly to the tests.
%Thanks for your advice!

[r5] Lin, X., Jiang, J., Ma, S., Zuo, Y., and Hu, C. One-pass trajectory simplification using the synchronous euclidean distance. VLDBJ 28, 6 (2019), 897–921.

[r6] Zhang, D., Ding, M., Yang, D., Liu, Y., Fan, J., and Shen, H. T. Trajectory simplification: An experimental study and quality analysis. PVLDB 9, 11 (2018), 934–946.

\textbf{[R1C10]} \emph{The average errors introduced in Page 20 are derived based on ONLY points contained in a line segment of a piece-wise line representation. Why? Why not consider all the points in the original trajectories? }

Indeed, we do consider ALL the points in the original trajectories when calculate the average errors. To clarify this, we have revised the description of ``average errors" in Section 6.2 to a more clear statement ``{The average simplification error is the average value of the distances from every point of the original trajectories to its representing line segment of the simplified trajectories.}``. Thanks for points out this!


\textbf{[R1C11a]} \emph{ In Page 21 (lines 18-19), it is stated that ``We then choose 10 trajectories from each dataset, and vary the size |T| of a trajectory from 1,000 points to 10,000 points while fixing the error bound $\epsilon = 40$ metres or $\epsilon = 45$ degrees". It is not clear how points are selected. For example, a raw trajectory contains 100 points (p1, p2, ..., p100). If we want to only consider 10 points, do you consider a sub-trajectory of 10 points (say p1, p2, ..., p10) or a trajectory of much lower sampling rate (say p1, p11, p21, ..., p91). Please state it clearly!}

It is the first K points of a trajectory (\eg, the first 100 points are $P_1, P_2, P_3, ..., P_{100}$). We have clarified this in the last paragraph of Section 6.1.

\textbf{[R1C11b]} \emph{ In Page 21 (lines 44 - 46), it is stated that ``The dataset collected by cars (e.g., UCar) also has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cares typically move more regularly than individuals." However, this is different from our everyday observation. Because of the traffic light, cars are not expected to move so regularly. Second, it is different from the observations we could make from Figures 15, 16, 17, 18, 19, and 20. Based on the results reported in those six figures, compression ratios under Geolife and Mopsi are lower than that under UCar, which means the dataset collected by cars has poorer compression ratios. Third, it shall be 'cars' but not 'cares'. }

The word ``regular", with respect to the movement of a car or an individual, has two meanings, say, ``regular in speed" and ``regular in direction". Roughly speaking, cars are expected to move more regular in direction (because they move along the roads) than individuals, while individuals move more regular in speed than cars (because cars have a broad range of speed and they frequently stop because of traffic light).
And partially because of this as well as the relatively lower average sampling rate of dataset UCar, it has poorer compression ratios than datasets Geolife and Mopsi when using Euclidean distance metrics PED and SED, as shown in Figures 16, 17, 19 and 20 (corresponding to Figures 15, 16, 18 and 19 of the previous manuscript). However, when using the direction metric DAD (recall cars are regular in direction), dataset UCar does have better compression ratios than Geolife and Mopsi, as shown in Figures 18 and 21 (corresponding to Figures 17 and 20 of the previous manuscript).

To clarify this, we have revised the sentence ``The dataset collected by cars (e.g., UCar) also has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cares typically move more regularly than individuals" to ``\textcolor{blue}{When using DAD,} the dataset collected by cars (e.g., UCar) has better compression ratios than the datasets partially collected by individuals (e.g., Geolife and Mopsi), as cars typically move more regularly than individuals \textcolor{blue}{in directions}" in the second paragraph of Section 6.3.1.
Thanks for pointing out these!


\textbf{[R1C12]} \emph{The impact of error bounds on compression ratios is straightforward. The bigger the error bounds, the better the compression ratio. However, the size of trajectory on the compression ratio is not that straightforward. Can the authors please comment on the observations we could make from Figures 18 -20? }

It seems that data size does not have remarkable impacts on compression ratios. For an online or one-pass algorithm that is compressing a trajectory, once some distance is greater than the error bound, it is sure to output a line segment and continue to compress the subsequent data points. As a result, data size does not have remarkable impacts on compression ratios. For batch and the optimal algorithms, our tests reveal a similar result.
%More specifically, a long trajectory is connected by a number of sub-trajectories $[\dddot{T}_1,\dddot{T}_2,..., \dddot{T}_m, ]$ that order by time, and each sub-trajectory $\dddot{T}_i, i \in [1,m]$, could be simplified to several line segments. Then suppose this original trajectory is being simplified by some online or one-pass algorithm that runs in an incremental manner, it is reasonable to conclude that the appending of $\dddot{T}_{m+1}$ might have some impacts of the representation (simplification) of $\dddot{T}_{m}$, but it's hard to have impacts on $\dddot{T}_i$ before $\dddot{T}_{m}$.

We have clarified this observation in the third paragraph of Section 6.3.1 (the 2{nd} item). Thanks!



\textbf{[R1C13]} \emph{ In Page 23 (lines 24 - 31), it is stated that ``given the same error bound $\epsilon$, the compression ratios of algorithms using PED are obviously better than using SED". This is actually misleading. Although the observation is that algorithms using PED could achieve higher compression ratios, the statement ignores the fact the PED does not consider the temporal dimension at all while SED does. The comparison is not fair at all! If the higher compression ratio is achieved by ignoring the error in the temporal dimension, it has to be stated clearly.} 

First of all, this evaluation aims to summarize the mainstream error bounded trajectory simplification algorithms and distance metrics, analyze the features of them and highlight the advantages and limitations of each kind of technologies, and also conduct a systematic experimental study under a uniform quality criteria (including compression ratios, errors, running time, aging friendliness and query friendliness) for large-scale trajectory data, so as to provide a full picture of line simplification algorithms and distance metrics to users. In this context,

(1) we have summarized the major characters of algorithms and distance metrics, and highlight the advantages and limitations of distance metrics at the beginning of the paper (please refer to the ``algorithm taxonomy" and ``distance metrics" parts of Section 1), with respect to your advice of [R1C5]. And yes, PED largely treats a trajectory as a sequence of spatial data points, thus, it has better compression ratios than SED at a price of losing temporal information. This is also claimed in Section 1,

(2) we objectively evaluate these technologies under the uniform quality criteria, including compression ratios, errors, running time, aging friendliness and query friendliness, and the same setting and parameters. These tests and comparisons are fair, especially under the condition that we have claimed the features, advantages and limitations of those technologies, as shown in (1), and

(3) we agree with your that the higher compression ratio is not always the better. Moreover, a single measure is not enough to determine an algorithm or distance metric, \ie users need to consider multiple metrics to choose a correct algorithm and distance metric so as to satisfy their unique requirements (this is also your advice of [R1C6]).

We have clarified this in the penultimate paragraph of Section 6.3.1, say, ``{because SED saves temporal information while PED does not (remember the loss of temporal information may lead to unexpected results, \eg unbounded answer-errors to spatio-temporal queries)}, given the same error bound $\epsilon$, the compression ratios of algorithms using PED are obviously better than using SED." Also, please refer to [R1C5] and [R1C6] for more details. Thanks!

\textbf{[R1C14]} \emph{ In Page 27 (lines 44 - 45), it is stated that ``When using DAD, the running time from the smallest to the largest is one-pass algorithms Intersect and Interval, batch algorithms TP and DP, and online algorithm OPW." However, algorithms actually perform slightly different at various datasets. It's not accurate to state that online algorithm OPW is always the worst, as it actually performs much better than TP and DP in the dataset Geolife (as shown in Figure 32(2)).}

Yes, we have revised the statement, and this sentence is replaced by ``When using DAD, one-pass algorithms Intersect and Interval run prominently faster than batch algorithms TP and DP and online algorithm OPW." 
Thanks for pointing out this!

\textbf{[R1C15]} \emph{ When evaluating the running time of different algorithms under various error bounds, different algorithms demonstrate different trends. The authors might want to explain why algorithms change the trends in certain ways (e.g., why error bounds do not have any impact on the running time of SIPED, why TP, OPW, BQS incur longer running time as error bounds increase, why ......).}

Yes, different algorithms demonstrate different trends under various error bounds because of their distinct routines and principles. More specifically, error bounds do not have obvious impacts on the efficiency of SIPED and other one-pass algorithms, because a data point is processed only one time during the whole process; the running time of batch algorithm DP (TP) decreases (increases) with the increase of error bound due to the top-down (bottom-up) approach it applies; and so on. The detailed analysis of these trends are summarized in the ``analyses of LS algorithms" part of Section 6.3.3. 
%
We believe it is helpful for users to understand the characters of these algorithms, and to choose an appropriate algorithm to meet their needs.

\textbf{[R1C16a]} \emph{ Figures 33 - 38 report the average errors of where\_at queries based on trajectories compressed using different line simplification algorithms. The title of 'Evaluation of spatio-temporal queries' is misleading as only one type of queries is considered.} 

Yes, we have revised the title of ''Evaluation of spatio-temporal queries" to ``Evaluation of \emph{where\_at} queries". Thanks for pointing out this!

\textbf{[R1C16b]} \emph{In addition, it is necessary to consider \emph{when\_at} query as it is a critical building block for many applications too.} 

Yes, we have added tests on the \emph{``when\_at"} query in Section 6.3.5. Thanks for your advice!

\textbf{[R1C17]} \emph{ Based on the results reported in Figures 33 -38, PED and DAD are actually not proper distance metrics that should be considered when we apply line simplification to compress the trajectories and meanwhile want to preserve the utility of the compressed trajectories. Especially, Figure 21 (avg PED errors under different error bounds) and Figure 33 (avg PED query errors under different error bounds) have different trends (Figure 23 and Figure 35 have different trends too), which further demonstrates that PED and DAD are actually misleading. An algorithm that can achieve good performance in terms of PED/DAD might not be able to guarantee its performance in real applications. }

Yes, a line simplification algorithm using PED or DAD is not able to guarantee the error bounds of spatio-temporal queries, which makes it inappropriate in scenarios that spatio-temporal queries are required. However, they still have their usages, \eg in some trajectory clustering that cares about the shape of a trajectory rather than the detailed positions of an individual. We have highlighted this in the penultimate paragraph of Section 6.3.6. More detailed discussion please refer to [R1C5]. 
Thanks!

\textbf{[R1C18]} \emph{ This paper focuses on applying line simplification for trajectory compression. Given the fact that there are different ways to compress trajectories (e.g., [1], [2]), it might be very helpful to give a brief introduction on different trajectory compression techniques, which could help audience to understand the pros/cons of applying line simplification to compress trajectories. }

\emph{ [1] CiNCT: Compression and Retrieval for Massive Vehicular Trajectories via Relative Movement Labeling. Satoshi Koide, Yukihiro Tadokoro, Chuan Xiao, Yoshiharu Ishikawa. ICDE'2018.}

\emph{ [2] COMPRESS: A Comprehensive Framework of Trajectory Compression in Road Networks. Yunheng Han, Weiwei Sun, Baihua Zheng. TODS 2017.}


{Yes, we have added an introduction to the semantic-based methods (including methods based on road networks) in ``Appendix: Additional Trajectory Compression Algorithms". Thanks for your advice!} 
%The relationship between them and line simplification is also discussed.

%******************* reviewer 2 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 2.}

\line(1,0){100}

\textbf{[R2D1]} \emph{In page 2, the authors mention that “important aspects of trajectory simplification (compression ratios, running time and aging friendliness) are not systematically studied” for [41]. Not sure which “important aspects” the authors refer to, as I found running time and the trade-off between compression ratios and errors have been reported in [41].}

The ``important aspects" refer to compression ratios, running time and aging friendliness.

(1) Running time: we have noticed that [41] ([59] in the revised version) does show a table listing \emph{``compression time per trajectory point"} of each algorithm, however, we argue that it is not sufficient to show the efficiency of these algorithms as algorithms have different running time \wrt error bounds, and the running time of many algorithms is not linear to the size of a trajectory (thus it may cost different time on one point when the size of an input trajectory is different). Our tests in Section 3.3.3 also support this argument.
 
(2) Compression ratios: it does report the relationship between compression ratios and average errors (note, they are average errors, not error bounds), however, both the compression ratios and the average errors are not separately tested \wrt error bounds.

(3) Aging friendliness: the data aging problem is not covered in [41]. 

Besides, some important algorithms, \eg CISED and SIPED, are not covered in [41] (CISED is published after [41]), Thus, it is not sufficient to guide users to choose an appropriate algorithm and set a reasonable error bound. 

We have revised the motivation part of the manuscript to clarify this.
Thanks for pointing out this!


%Indeed, our study is a necessary complement to existing studies by providing a systematic evaluation and analyses of error bounded trajectory simplification algorithms. 
%Comparing with [41], this work has the following new findings: 
%(1) compression ratios and efficiency of optimal, batch, online and one-pass algorithms are systematically evaluated \wrt distance metrics (PED, SED and DAD), error bounds and data sizes, as summarized in the Section 6.3.6, 
%(2) aging friendliness and safety are comprehensively evaluated, as reported in Section 6.3.4,
%(3) though online algorithm DOTS shows better trade-off between compression ratio and average error [41], it is not effective in terms of compression ratios. Besides its Java implementation is not efficient in large datasets, and 
%(4) one-pass algorithm SIPED ($\epsilon$) and one-pass algorithm CISED, either CISED($\epsilon$) or CISED-W, are efficient as well as has good compression ratios.

\textbf{[R2D2]} \emph{In page 3: $\epsilon_1$ and $\epsilon_2$ appeared without explanation.}

Yes, we have fixed it, say, ``{where $\epsilon_1$ and $\epsilon_2$ are the error bounds set in the first and second times of simplification, respectively.}" 
Thanks!

\textbf{[R2D3]} \emph{ In page 4: It’s better to move the comparison with [41] to the motivation part, in order to assure the necessity of a new empirical study.}

Yes, we have moved some comparisons with [41] to the motivation part. Indeed, the very recent study [41] does evaluate a wide range of trajectory simplification algorithms.
However, it provides {an experimental study} on compression errors and spatio-temporal query analyses only, and important aspects of trajectory simplification (compression ratios, running time, aging friendliness and safety) are not systematically studied. 
{More specifically,}
{(1) it has a limited evaluation on running time: it reports the \emph{compression time per trajectory point} of each algorithm on full datasets, however, an algorithm has different running time \wrt error bounds and the running time of many algorithms are not linear to the size of a trajectory (thus it may cost different time on one point when the input trajectories have different sizes), meaning the impacts of both the sizes of trajectories and the error bounds on the running time should be tested;}
%
{(2) it has a limited evaluation on compression ratios: it does report the relationship between compression ratios and average errors, but the impacts of the error bounds on the compression ratios are not separately tested; and }
%
(3) it does not cover the \emph{data aging} problem.
%
{Besides, some important algorithms, \eg optimal algorithms using PED and SED and one-pass algorithms SIPED and CISED, are not investigated in [41]. 
%


Thanks for your advice!
%that is helpful to elaborate the motivation of the work

\textbf{[R2D4]} \emph{The authors mention that one of the main contributions is to re-implement the methods with Java. So compared with the running time results reported in [41], any new or different findings are revealed?}

Yes, we have found that algorithm DOTS runs slowly in Java when the input trajectory is large (its C/C++ version is fast, as reported in [41]), partially because in Java the frequent copies of memory waste a lot of time and become its bottleneck of efficiency.




We have clarified this in Section 6.3.6, say, `` When DOTS~is processing long trajectories, it needs to frequently copy memory, this will make it even slower than batch algorithms if it is implemented in Java." 
%Thanks


%First of all, we re-implement the methods with the same language, Java, for a fair comparison.


\textbf{[R2D5]} \emph{In Table 1, what are the criteria for the selection of “representative” algorithms? The authors may need to explicitly explain this in this paper.}

We comprehensively consider the performance of algorithms, the supporting of distance metrics and the novelty of their key ideas to select the representative algorithms. More specifically, 
(1) for optimal algorithms, {as all the optimized optimal algorithms have the same effectiveness using the same distance metric, and essentially work for small size trajectories only, we choose algorithm Optimal that supports PED, SED and DAD as the representative of optimal line simplification algorithms,}
%
(2) for batch algorithms, {they basically work for small and medium size trajectories, and we choose top-down algorithm DP and bottom-up algorithm TP that all support PED, SED and DAD as the representatives of batch algorithms,}
%
(3) for online algorithms, {we choose algorithms BQS, SQUISH-E and DOTS that are optimized for PED, SED and LISSED, respectively, as the representatives of online algorithms. Because no specific techniques have been developed for DAD, the classic OPW is chosen for it,} and
%
(4) for one-pass algorithms, OPERB, SIPED, CISED, Intersect and Interval are effective. Among them, algorithms OPERB and SIPED are specific to PED, CISED is specific to SED, and Intersect and Interval are specific to DAD. We choose them as the representatives of one-pass algorithms.

To clarify these, we have added a brief explanation of the criteria after Table 1, and explained the details selections in the top paragraphs of Sections 3, 4.1, 4.2 and 4.3. 
%Thanks!

\textbf{[R2D6]} \emph{Obviously, data aging is a very important contribution in this paper, because it was not examined in [41]. However, the situation becomes very awkward when most of the existing algorithms are not data aging friendly. Among all the methods in Table 1, only DP (with SED and PED as the distance measure) is data aging friendly. (1) {If a user considers data aging is important, then he/she has no choice but to use DP}. (2) {The authors may need to provide more convincing arguments for the necessity of examining data aging}.}



(1) \underline{``If a user considers data aging is important, then he/she has no choice but to use DP"}: \emph{Data aging} is important while \emph{aging friendly} is not that important, say, \emph{data aging} is rather an important requirement derived from the management of trajectory data, and it is not equivalent to \emph{aging friendly}. Thus, when we talk about data aging, it is not necessary to choose the \emph{aging friendly} algorithm (DP) to compress trajectories --- though given the same error bounds, it may bring better compression ratios in data aging. Also notice that compression ratios is only one consideration on choosing an algorithm, say, there are also errors and efficiency that impact the selection of a line simplification algorithm.
%That is to say, any algorithm could be used in data aging. 

(2) \underline{``The authors may need to provide more convincing arguments for the necessity of examining data aging"}: Majorly, we have proved two important things \wrt data aging, (a) most algorithms are not \emph{aging friendly} (Section 5.1), and (b) all algorithms are \emph{aging safe} (Section 5.2), \ie let $\mathcal{A}$ be a line simplification algorithm,  $\mathcal{M}$ be a distance metric, and $\epsilon_1>0$ and $\epsilon_2>0$ be error bounds, algorithm $\mathcal{A}$ is \emph{aging safe} if the errors between original trajectory $\dddot{\mathcal{T}}$ and simplified trajectory $\overline{\mathcal{T}}=\mathcal{A}(\mathcal{A}(\dddot{\mathcal{T}}, \epsilon_1, \mathcal{M}), \epsilon_2, \mathcal{M})$ are not more than $\epsilon_1+ \epsilon_2$. Which means that, in data aging, \emph{one can freely re-compress trajectories by any algorithm as long as he uses the same distance metric.} The aging friendliness and aging safety of algorithms (Table 3) are two important findings of the work, these together provide a full picture of the data aging problem, and let user know how to choose an algorithm and set the parameter of error bound in data aging. 
%, and also serves as the reason of examining data aging

We have clarified these in Section 5. Thanks for pointing out this!

\textbf{[R2D7]} \emph{In this paper, only three types of distance measure PED, SED and DAD are presented and compared. The other distance measures are briefly touched in the Appendix. Question here is that method like Dots is recommended for online compression in [41] as it shows better trade-off in terms of compression ratio and error. So when recommending users with suitable compression algorithms, would the guidance be biased if the authors exclude certain existing methods?}

Yes, we have included the local integral square SED (LISSED) and evaluated algorithms OptLISSED, MRPA, DOTS and OLTS using LISSED in the revised version. In particular, we have (1) briefly introduced LISSED in Section 2, after the definition of the Synchronous Euclidean Distance (SED), (2) selected DOTS as the representative of these algorithms and described it in Section 4.2.4, and (3) tested DOTS and reported our findings about DOTS and LISSED in Section 6.

Thanks for your advice!

%Also note that, in this revision, {the LISSED is rather a special SED-based ``error measure" or a methodology that efficiently calculate SED errors than a kind of ``distance measure"}.

\textbf{[R2D8]} \emph{In page 23: the authors mentioned that “in practice, SED has obviously better compression ratios than DAD..”. This is confusing to me because the distance unit for SED is meter and unit for DAD is degree. How can they be compared? It seems like the effect of epsilon=100m in SED is similar to epsilon=60 degree in DAD?}

As you have mentioned, SED is a Euclidean distance metric, having a value in $[0, \infty]$, and DAD is a direction metric, having a value in $[0, 360]$ degrees, hence, it is hard to compare them under absolutely fair conditions. However, we believe it is helpful to give readers an intuitive impression about the performance of DAD compared with SED. Thus, we alternatively suppose there are two practical scenarios, one uses SED with $\epsilon  \le  100$ meters, and the other uses DAD with $\epsilon \le 60$ degrees, then we compare the performance of them, \eg the performance of SED with $\epsilon=100/k$ meters vs. that of DAD with $\epsilon=60/k$ degrees ($k\ge 1$). 


{We have revised the last paragraph of Section 6.3.1 to clarify this. }
%Thanks!

\textbf{[R2D9]} \emph{Besides clarifying the differences with [41] in the introduction, it’s better to summarize the differences of experimental findings at the end of the paper, e.g., from the experimental results, what are the new insights not covered in [41] and what are the different/conflicting findings?}

Comparing with the recent experimental study [41], this work has the following new findings: 
	(1) compression ratios and efficiency of optimal, batch, online and one-pass algorithms \wrt distance metrics (PED, SED and DAD), error bounds and data sizes are summarized in Section 6.3.6, 
	(2) aging friendliness and safety are reported in Section 6.3.4,
	(3) though online algorithm DOTS shows good trade-off between compression ratio and average error [41], it is not effective in terms of compression ratios. Besides its Java implementation is not efficient in large datasets,  
	(4) one-pass algorithm SIPED ($\epsilon$) is efficient as well as has good compression ratios, and 
	(5) one-pass algorithm CISED, either CISED ($\epsilon$) or CISED-W, is better than batch and online algorithms in terms of compression ratios and efficiency.

We have summarized these at the end of Section 6.3.6. Thanks for pointing out this!


%******************* reviewer 3 ***********************************************
\line(1,0){500}

\textbf{Response to the comments of Reviewer 3.}

\line(1,0){100}

\textbf{[R3C1a]} \emph{
Sec. 4.1. The min-\# optimal version from [4] has different complexities depending on whether the polyline is open or closed (as well as convex vs. concave). The authors have not properly brought this in context.}

Yes, [4] proves the \emph{min-$\#$} problem (using PED) for a general polygonal curve can be solved in $O(n^2)$ time by using the \textit{sector intersection} mechanism, and for some special curve, either open or closed (if there is an edge joining the first and the last points, then it is closed, otherwise, it is open) polygonal curve that forms part of a convex polygon, this problem can be solved in $O(n)$ time. Note that, in general, a trajectory is not necessary a convex or closed polygonal curve, instead, it is often open and concave. Thus, the best optimal algorithm for trajectory simplification using PED still has $O(n^2)$ time.} 

We have revised the second paragraph of section 3 to clarify this. Thanks for pointing out this!

\textbf{[R3C1b]} \emph{Also, in multiple places, the authors are bringing the notion of the ``3D" - however, they do not mention the reference [r1] which was the first one to extend the algorithm in [4] to 3D. }

\emph{[r1] Gill Barequet, Danny Z. Chen, Ovidiu Daescu, Michael T. Goodrich, and Jack Snoeyink. Efficiently approximating polygonal paths in three and higher dimensions. Algorithmica, 33(2):150–167, 2002.}

Yes, we have observed that in areas of image processing, computer graphics and cartography, there is another 3D space, \ie \emph{x-y-z} 3D space, where {\em z} is height, and some works solve the \emph{min-\#} problem in the \emph{x-y-z} 3D space. Among them, authors in [r1] also extend the \textit{sector intersection} method to the \textit{off-line ball-inclusion testing} in the \emph{x-y-z} 3D space, such that they are able to develop efficient near-quadratic-time algorithms in three dimensions for solving the \emph{min-\#} and \emph{min-$\epsilon$} problems.} %

We have clarified this in the first paragraph of Section 4.3.3. Thanks!

\textbf{[R3C2]} \emph{
Sec. 4.2. One aspect that is context-dependent (and often implicitly ignored) in online algorithms, is the communication overhead (which, in turn, may imply energy consumption in sensor networks); along with the complementary aspect of ``freshness" of the compressed data in the respective sink. Please see [r2] below.}

\emph{[r2] Oliviu Ghica, Goce Trajcevski, Ouri Wolfson, Ugo Buy, Peter Scheuermann, Fan Zhou, Dennis Vaccaro: Trajectory Data Reduction in Wireless Sensor Networks. IJNGC 1(1) (2010)
}

{Yes, in Wireless Sensor Networks, online algorithms should address the problem of balancing the trade-off between the energy cost due to communication and the accuracy of the trajectories’ detection and representation [r2].}

We have clarify this in the first paragraph of Section 4.2. 
Thanks!

\textbf{[R3C3a]} \emph{
Sec. 4.3. Actually [13] does provide a kind of a taxonomy, and it would be nice to compare it with the present article. }

Indeed, our taxonomy (batch, online and one-pass sub-optimal line simplification algorithms) is the same as [13] (respectively, offline, online and real-time algorithms). 
We prefer the words ``batch" and ``one-pass" in that ``batch" is a customary name in trajectory compression and ``one-pass" better catches the common feature of the applied \emph{distance checking policies} of those algorithms.  

We have clarified this in the 3rd paragraph of Section 1 (algorithm taxonomy). 
%Thanks for your advice!

\textbf{[R3C3b]} \emph{Also, [37] has a claim/proof that if the online version uses $\varepsilon$ as an error-bound in the LDR, then the final outcome is an equivalent to a compression of $2 \cdot \varepsilon$ applied to the entire historic trajectory (if it were available). This, in a sense, is a form of a justification for the use of $\varepsilon$/2 in works like CISED ([14]).}

{Yes, [37] proves that if LDR uses $\epsilon/2$ as the error-bound in position tracking, then its output trajectory has a max error not greater than $\epsilon$ to the original trajectory. Thus, it's a trajectory simplification algorithm as well as a position tracking protocol.} Besides, its idea of half $\epsilon$ is earlier than CISED and Intersect.

We have stated this in the 3rd and 5th paragraphs of Section 4.3. 
%Thanks for pointing out this!

\textbf{[R3C4]} \emph{
In several places, the authors are making statements that are not properly phrased, and some that can be taken out of context. Examples:}

\emph{1. The authors should try to define terms before their first use. Example: on p.1 in the Introduction, the ``min-\#" is brought up, without properly explaining its meaning.}


\emph{2. p.2: ``...no need of extra knowledge and suitable for freely moving objects [28]...". A statement of this sort does not do justice to [28].
Namely, [28] considers what happens with compression if the trajectories are constrained to move along road networks - but, it also is based on the motivation that in such settings there may be a different perspective - i.e., the overall-compression of the DB.}


\emph{3. p.33: the authors make a claim to the effect of: ``These methods lack the capability of compressing..." when describing the min-$\epsilon$ variant. The statement needs to be clarified, or put in the more appropriate contexts: the min-$\epsilon$  is a complementary one to min-\# and, as such, it never makes any other claim that it will yield the minimum error, given a fixed ``budget" of m points. However, that does not mean that this variant of the problem, and the algorithmic solution, doesn't have its own merits.}



(1) Yes, the \emph{min-\#} problem of trajectory simplification is, given a curve (\ie an original trajectory) and an $\epsilon$, to construct an approximate curve with error within $\epsilon$ and, at the same time, having the minimum number of line segments. We have explained it in Section 1 (the second paragraph). 

(2) Yes, we have replaced the sentence to ``light and applicable to resource-constrained devices", in Section 1 (the second paragraph). 

(3) Yes, we have revised these sentences to ``the \emph{min-$\epsilon$} problem: given $m$, constructs an approximate curve consisting of at most $m$ line segments with the minimum error. Algorithms like SQUISH ($\lambda$) and SQUISH-E ($\lambda$) solve this problem and are the complementary ones to those solving the \emph{min-$\#$} problem." Besides, the introduction to the \emph{min-$\epsilon$} problem is moved to the second paragraph of Section 1.

Thanks for your advice!

\line(1,0){500}



Your sincerely,

Anonymous authors
%Xuelian Lin, Shuai Ma, Yanchen Hou, Yihao Fu and Tianyu Wo

%\bibliographystyle{abbrv}
%\bibliography{sec-ref}


\end{document}
